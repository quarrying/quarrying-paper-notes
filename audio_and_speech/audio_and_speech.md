## [2017] VGGish
----
- https://github.com/tensorflow/models/tree/master/research/audioset/vggish

## [2019] PANN
---
- [2019] PANNs_ Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition
- https://github.com/qiuqiangkong/audioset_tagging_cnn

## [2020] wav2vec
----
- [2020] wav2vec 2.0: A framework for self-supervised learning of speech representations

## [2021] Audio ALBERT
---
- [2021] Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation


# Datasets

## AudioSet
---
> Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.

- https://research.google.com/audioset/index.html
- [2017] Audio Set: An ontology and human-labeled dataset for audio events

## MSD, Million Song Dataset
---
- http://millionsongdataset.com/

