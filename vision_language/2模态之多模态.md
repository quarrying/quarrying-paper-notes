# Language-Image Alignment

## [2021 ICML] CLIP, Contrastive Language-Image Pre-training
----
CLIP 自行构建了一个新的数据集 WebImageText, 简称为 WIT (WIT-400M).

CLIP 使用 image-text matching 作为预训练任务. 损失函数称为 image-text contrastive loss, 简称为 ITC.

CLIP uses an autoregressive text encoder (GPT-2), a BytePairEncoding tokenizer, and a length of 77.

CLIP image encoder use ViT-L/14@336px which found to perform best.

generative objective 比 contrastive objective 更耗时.

- [2021 ICML] Learning Transferable Visual Models From Natural Language Supervision
- https://github.com/openai/CLIP

## [2021 ICML] ALIGN, A Large-scale ImaGe and Noisy-text embedding
----
> Besides using different vision and language encoder
> architectures, the key difference is on training data: ALIGN
> follows the natural distribution of image-text pairs from the
> raw alt-text data, while CLIP collects the dataset by first
> constructing an allowlist of high-frequency visual concepts
> from English Wikipedia.

ALIGN 和 CLIP 并无太大差异, 主要差异在于训练集数据构造方式.

- [2021 ICML] Scaling up visual and vision-language representation learning with noisy text supervision

## [2022] Chinese CLIP
---
预训练训练集包括: LAION-5B 的中文部分 (108 million samples), Wukong (72 million samples) 及一些英文数据集的翻译, 如 Visual Genome 和 MSCOCO. 最终训练集大小为 200 million 图文对.

- [2022] Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese
- https://github.com/ofa-sys/chinese-clip
- https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16

## [2022] OpenCLIP
---
基于开源的 LAION 数据集和 OpenCLIP 代码库来研究 CLIP 的 scaling laws, 并开源了训练代码及结果模型.

- https://github.com/mlfoundations/open_clip
- [2022] Reproducible scaling laws for contrastive language-image learning

## [2024] jina-clip-v2
---
- [2024] jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images

## [2025] TULIP
----
- [2025] TULIP: Towards Unified Language-Image Pretraining

# Pre-training and fine-tuning

## [2019] VisualBERT
---
- [2019] VisualBERT: a simple and performant baseline for vision and language


## [2021] ALBEF
---
损失函数包括 ITC (image-Text Contrastive) loss, ITM (Image-Text Matching) loss, MLM (Masked Language Modeling) loss.

- [2021] Align before Fuse_ Vision and Language Representation Learning with Momentum Distillation

## [2021] VLMO
----
- [2021] VLMo_ Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

## [2022 ICML] OFA
---
- [2022 ICML] OFA_ Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework

## [2022] LiT, Locked-image text Tuning
----
简言之: 图像编码器用预训练模型初始化且冻结, 文本编码器随机初始化且可训练, 任务为 Contrastive pre-training (和 CLIP 一样).

> This indicates that locking the image tower during tuning, i.e. LiT, leads to a text model that is well aligned to an already strong and general image representation, as opposed to an image-text model that is well aligned but specialized to the dataset used for alignment.

- [2022] LiT_ Zero-Shot Transfer with Locked-image text Tuning


## [2022 CVPR] UniCL
---
简言之: 建立 image, text 和 label 的语义对齐?

- [2022 CVPR] Unified Contrastive Learning in Image-Text-Label Space
- https://github.com/microsoft/UniCL

## [2022] XCLIP, X-CLIP, X-Florence
----
在 CLIP 模型上添加若干模块, 用于视频分类.

- [2022] Expanding Language-Image Pretrained Models for General Video Recognition

## [2022] FLIP, Fast Language-Image Pre-training
---
- [2022] Scaling Language-Image Pre-training via Masking

## [2022] VLKD, vision-language knowledge distillation
---
- [2022] Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation

## [2021] ViLD, **Vi**sion and **L**anguage knowledge **D**istillation
----
利用 CLIP 来实现零样本目标检测, 具体地: 使用 CLIP 的 text encoder 提取标签的特征作为 classifier weight, 同时利用 CLIP 的 image encoder 提取 proposal 特征来蒸馏 detector rpn 提取的 proposal 特征.

- [2021] Zero-shot detection via vision and language knowledge distillation
- https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild

## [2021] RegionCLIP
----
文章通过实验发现: CLIP 的 image encoder 提取的特征的定域性很差 (紧贴的框比扩边的框的效果还差), 不适用于目标检测任务适配. 

本文利用 CLIP 构建了 region-text pair 数据集, 并使用 region-text matching 作为预训练任务.

该文没有对语言模型进行训练:
> Our method relies on CLIP’s visual-semantic space and has not updated the language encoder. When given similar scale of data as CLIP, unfreezing the language encoder may bring more gain in our region-based language-image pretraining.

- [2021] RegionCLIP_ Region-based Language-Image Pretraining
- https://github.com/microsoft/RegionCLIP

## [2022] DenseCLIP
---
将 CLIP 适配于稠密预测任务. 具体思路: 将 CLIP 中的 image-text matching 任务转化为 pixel-text matching 任务, 并通过 context-aware prompting 来提高文本特征的表示能力.

- [2022] DenseCLIP_ Language-Guided Dense Prediction with Context-Aware Prompting
- https://github.com/raoyongming/DenseCLIP

## [2023] segment-anything-with-clip
---
将 CLIP 和 SAM 适配于语义分割. 具体思路: 利用 SAM 提取目标区域, 利用 CLIP 的 image encoder 提取目标区域的特征, 并利用 CLIP 的 text encoder 提取标签特征.

- https://github.com/Curt-Park/segment-anything-with-clip
- https://huggingface.co/spaces/curt-park/segment-anything-with-clip

## [2021] GLIP, Grounded Language-Image Pre-training
---
GLIP 利用 phrase grounding 作为预训练任务, 该任务比 image-text matching 任务的监督信号更强. 该文献统一了 phrase grounding 和 object detection (实际上是将 object detection 任务转化为 phrase grounding 任务).

论文提出两个数据集: Cap4M 和 Cap24M, 但没有构建细节.

- [2021] Grounded Language-Image Pre-training
- https://github.com/microsoft/GLIP

## [2023] MaskCLIP
---
提出的预训练任务集合了 image-text matching, masked self-distillation 和 local semantic learning.

- [2023] MaskCLIP_ Masked Self-Distillation Advances Contrastive Language-Image Pretraining
- https://github.com/LightDXY/MaskCLIP 

## [2022] BLIP, Bootstrapping Language-Image Pre-training
----
损失函数包括 ITC (image-Text Contrastive) loss, ITM (image-Text Matching) loss, MLM (Masked Language Modeling) loss.

BLIP 提出一个名为 CapFilt (Captioning and Filtering) 的方法, 用来构建和清洗 captioning 数据.

> An MED can operate either as a unimodal encoder, or an image-grounded text encoder, or an image-grounded text decoder.

- [2022] BLIP_ Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- https://github.com/salesforce/BLIP

## [2021] Florence
----
- [2021] Florence: A New Foundation Model for Computer Vision

## [2022] Flamingo
----
基本思想: 图像预训练模型可训练, 文本预训练模型冻结, 任务为 vision-to-language generation?

- [2022] Flamingo_ a Visual Language Model for Few-Shot Learning

## [2023] Openflamingo
---

## [2023] BLIP-2
----
基于 FLAN-T5-XXL, 重要组件: Q-Former (Querying Transformer). 

- [2023] BLIP-2_ Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

## [2023] MiniGPT-4
---
基于 Vicuna-7B.

- [2023] Minigpt-4: Enhancing vision-language understanding with advanced large language models

## [2023 NeurIPS] LLaVa, Large Language and Vision Assistant
---
论文的一个贡献是: 提出了一个自动化构建 language-image instruction-following data 的方法.

该论文利用了上述的方法, 构建了一个视觉指令微调数据集: LLaVA-Instruct. `LLaVA-Instruct is composed of the above three types of visual instructions, including 23K image captions, 58K conversations about visual questions, and 77K visual reasoning questions.`

LLaVa 语言模型基于 LLaMA, 视觉模型基于 CLIP 中的视觉编码器 (ViT-L/14).

- [2023 NeurIPS] Visual instruction tuning
- https://llava-vl.github.io/
- https://github.com/haotian-liu/LLaVA


## [2023] LLaVa 1.5
---
使用 MLP 替换简单的线性层.

- [2023] Improved Baselines with Visual Instruction Tuning


## [2024] LLaVA-NeXT
---
- https://llava-vl.github.io/blog/2024-01-30-llava-next/


## [2023] KOSMOS-1
----
- [2023] Language is not all you need: Aligning perception with language models

## [2023] Multimodal-gpt
---
- [2023] Multimodal-gpt: A vision and language model for dialogue with humans

## [2023] mPLUG, mplug-owl
---
- [2023] mplug-owl: Modularization empowers large language models with multimodality

## [2023] Internlm-xcomposer
----
- [2023] Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition

## [2023] InstructBLIP
---
- [2023] InstructBLIP: Towards general-purpose vision-language models with instruction tuning

## [2023] Qwen-vl
---
- [2023] Qwen-vl: A frontier large vision-language model with versatile abilities

## [2023] Shikra
---
- [2023] Shikra: Unleashing multimodal llm's referential dialogue magic

## [2023] PaLM-E
---
- [2023] PaLM-E: An embodied multimodal language model

## [2023] ComVint
---
论文将 visual instruction collections 归纳为三种任务类型: image captioning, VQA, visual reasoning.

- [2023] What Makes for Good Visual Instructions_ Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning
- https://github.com/RUCAIBox/ComVint

## [2023] Kosmos-2
---
提出一种自动化构建 Grounded Image-Text pairs 数据集的方法, 并用该方法构建了名为 GRIT (Grounded Image-Text) 的数据集.

- [2023] Kosmos-2: Grounding multimodal large language models to the world

## [2023] Otter
---
- [2023] Otter_ A multi-modal model with in-context instruction tuning

## [2023] Ferret
---
- [2023] Ferret_ Refer and Ground Anything Anywhere at Any Granularity

## [2023] LLaVA-Grounding
---
- https://llava-vl.github.io/llava-grounding/
- https://github.com/UX-Decoder/LLaVA-Grounding
- https://llava-grounding.deepdataspace.com/
- [2023] LLaVA-Grounding_ Grounded Visual Chat with Large Multimodal Models

## [2024] Qwen2-VL
---
- Qwen2-VL-2B
- Qwen2-VL-7B
- Qwen2-VL-72B

- [2024] Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution

## [2024] Intern-VL 2.5
---
- [2024] Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling


# Captioning

## [2023] CapsFusion
---
本文在 LAION-COCO 基础上, 提出了 CAPSFUS-120M 数据集.

- [2023] CapsFusion_ Rethinking Image-Text Data at Scale

## [2025] URECA
---
区域级的 captioning.

- [2025] URECA: Unique Region Caption Anything

## [2025] OVFact
----
提出一种评价 caption 模型 Factuality 的一种方法, 及一种 caption 数据的清洗方法.

- [2025] OVFact_ Measuring and Improving Open-Vocabulary Factuality for Long Caption Models


# Text-to-image generation

## [2022] DALL·E, DALLE, DALL·E 2, DALL·E 3
----
DALL·E 中使用了 CLIP.

- [2022] Hierarchical text-conditional image generation with clip latents

## [2022 NIPS] Imagegen
---
- [2022 NIPS] Photorealistic text-to-image diffusion models with deep language understanding

## [2022] Parti
---
- [2022] Scaling autoregressive models for content-rich text-to-image generation

## [2022] Textual-Inversion
----
- [2022] An image is worth one word: Personalizing text-to-image generation using textual inversion

## [2022] DreamBooth
----
- [2022] Dreambooth_ Finetuning text-to-image diffusion models for subject-driven generation
- <https://huggingface.co/spaces/carlgira/dreambooth-image-editor>

## [2022] SINE, SINgle Image Editing
----
- [2022] SINE_ SINgle Image Editing with Text-to-Image Diffusion Models

## [2021] Cogview
---
- [2021] Cogview: Mastering text-to-image generation via transformers

## [2021] LAFITE
----
- [2021] LAFITE: towards language-free training for text-to-image generation

## [2021] Make-a-scene: Scene-based text-to-image generation with human priors
-----

## [2021] GLIDE
----
- [2021] GLIDE: towards photorealistic image generation and editing with text-guided diffusion models

## [2023] ImageBind
---
- [2023] ImageBind: One Embedding Space To Bind Them All


# Datasets & Challenges

## Person In Context
----
- http://picdataset.com/challenge/index/

## ActivityNet Captions
----

## Charades-STA
----
- [2016 ECCV] Hollywood in homes: Crowdsourcing data collection for activity understanding

## Visual Genome, VG
----
- [2017 IJCV] Visual genome_ Connecting language and vision using crowdsourced dense image annotations
- https://visualgenome.org/

## Conceptual Captions, CC, CC3M
----
该文献提出了一个 image-text 数据集的构造方法, 这个方法在后续的文献中并广泛借鉴 (例如 ALIGN).

- [2018 ACL] Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning
- https://github.com/google-research-datasets/conceptual-captions

## SBU Captions
----
- [2011 NeurIPS] Im2text: Describing images using 1 million captioned photographs
- http://dsl1.cewit.stonybrook.edu/~vicente/sbucaptions/

## COCO captions
----
> containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

1 张图像对应 5 段文本, 文本比较简短, 平均只有 52 个字符.

- [2015] Microsoft COCO captions: Data collection and evaluation server

## TextCaps
----
- [2020 ECCV] TextCaps: a Dataset for Image Captioningwith Reading Comprehension

## nocaps, novel object captioning at scale
----
- https://nocaps.org/

## LAION, LAION-400M, LAION-5B, LAION-2B
----
LAION-2B: an English image-text subset of 2.32 billion samples of LAION-5B.

- [2021] Laion-400m_ Open dataset of clip-filtered 400 million image-text pairs
- [2022] LAION-5B_ An open large-scale dataset for training next generation image-text models
- https://laion.ai/blog/laion-5b/


## Wukong, Noah-Wukong
-----
华为诺亚方舟开源的中文多模态数据集. 

The dataset contains 100 Million image-text pairs

- https://wukong-dataset.github.io/wukong-dataset/

## Flickr8k, Flickr8k-CN, Flickr 8k
-----
- [2013] Framing image description as a ranking task: Data, models and evaluation metrics
- [2016] Adding Chinese Captions to Images

## Flickr30k, Flickr 30k, Flickr30K-CN
-----
图文检索数据集, 目前指标已经刷的很高了.

> an image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. This is an extension of our previous Flickr 8k Dataset. The new images and captions focus on people involved in everyday activities and events.

- [2014] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
- https://shannon.cs.illinois.edu/DenotationGraph/

## Flickr30k entities
----
Flickr30k entities 是 Phrase grounding 数据集.

- [2017 IJCV] Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models
- https://bryanplummer.com/Flickr30kEntities/

## Conceptual 12M, CC12M
---
> CC12M is on purpose designed for vision-and-language pre-training, and meant to be disjoint from CC3M. CC3M is cleaner and more appropriate for fine-tuning, but can be used in conjunction with CC12M for pre-training, as illustrated in our paper. Coincidentally, their intersection is found to be non-zero — approximately 63K URLs.

- [2021 CVPR] Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts
- https://github.com/google-research-datasets/conceptual-12m


## COYO-700M, COYO-Labeled-300M
----
> COYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models.
> We also provide COYO-Labeled-300M by adding machine-generated vision labels to a subset of COYO-700M for comparison with the JFT-300M.

- https://github.com/kakaobrain/coyo-dataset
- [2022] COYO-700M: Image-Text Pair Dataset 


## WIT, Wikipedia-based Image Text Dataset
----
> Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages.

注意 CLIP 的训练集也成为 WIT (WebImageText).

- https://github.com/google-research-datasets/wit
- [2021] WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning


## YFCC100M-CLIP
---
> YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automatically generated filenames like 20160716_113957.JPG as “titles” or contain “descriptions” of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos.

- [2021] Learning transferable visual models from natural language supervision


## MUGE, Multimodal Understanding and Generation Evaluation Benchmark
---
- https://tianchi.aliyun.com/muge

## WaveCaps
----
> which contains 403,050 audio clips with average duration of 67.59 seconds and average caption length of 7.8 words. It combines four datasets including FreeSound (262,300) [31], BBC Sound Effects (31,201), SoundBible (1,231) and AudioSet strongly-labelled subset (108,317), and transform their raw-descriptions into captions with ChatGPT.

- [2023] Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research

## Zero
----
> Zero, a large-scale Chinese cross-modal benchmark, contains two pre-training datasets called Zero-Corpus and five downstream datasets.

- https://zero.so.com/index.html

## Laion coco, LAION-COCO
---
> contains both raw captions from the web and synthetic captions generated by BLIP.

- https://laion.ai/blog/laion-coco/

## A-OKVQA
---
- [2022 ECCV] A-OKVQA: A benchmark for visual question answering using world knowledge

## SEED-Bench
---
- [2023] Seed-bench: Benchmarking multimodal llms with generative comprehension

## MMBench
---
- [2023] Mmbench_ Is your multi-modal model an all-around player

## MME, MME-Cognition
----
> MME aims to measure both perception and cognition abilities of MLLMs, where perception evaluation (MME-Perception) includes coarse-grained tasks, fine-grained tasks, and OCR tasks; cognition evaluation (MME-Cognition) includes commonsense reasoning, text translation, numerical calculation, and code reasoning. In MME, each instance consists of one image and two yes-or-no questions.

- [2023] MME: A comprehensive evaluation benchmark for multimodal large language models

## OK-VQA
----
- [2019 CVPR] OK-VQA: A visual question answering benchmark requiring external knowledge

## RefCOCO, RefCOCO+, RefCOCOg
---
REC 数据集. RefCOCO, RefCOCO+ 和 RefCOCOg 三个数据集合称为 RefCOCO/+/g.

RefCOCO: Contains 142,209 refer expressions for 50,000 objects across 19,994 images. 

RefCOCO+: Includes 141,564 expressions for 49,856 objects in 19,992 images. 

RefCOCOg: This variant has 25,799 images, 95,010 referring expressions, and 49,822 object instances.


- RefCOCO: [2014 ECCV] Microsoft coco: Common objects in context
- RefCOCO+: [2016 ECCV] Modeling context in referring expressions
- RefCOCOg: [2016 CVPR] Generation and comprehension of unambiguous object descriptions
- https://github.com/lichengunc/refer

## GoldG
--- 
> GoldG, 0.8M human-annotated gold grounding data curated by MDETR [23], including Flickr30K, VG Caption [28], and GQA [19].

- [2021] Mdetr-modulated detection for end-to-end multi-modal understanding
- https://github.com/ashkamath/mdetr

## [2022] ScienceQA
---
- [2022] Learn to explain: Multimodal reasoning via thought chains for science question answering
- https://scienceqa.github.io/


## [2024] Panda-70M
---
- [2024] Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers


## ShareGPT4V, ShareGPT4V-PT
---
ShareGPT4V 数据集是借助 GPT4-Vision (GPT4v) 得到的, 有 10 万条.

> In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data. To address this bottleneck, we introduce the ShareGPT4V dataset, a pioneering large-scale resource featuring 1.2 million highly descriptive captions, which surpasses existing datasets in diversity and information content, covering world knowledge, object properties, spatial relationships, and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated 100K high-quality captions collected from advanced GPT4-Vision and has been expanded to 1.2 million with a superb caption model trained on this subset. ShareGPT4V first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT) phase, by substituting an equivalent quantity of detailed captions in existing SFT datasets with a subset of our high-quality captions, significantly enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and 2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks. We will release this project to serve as a pivotal resource for advancing the LMMs community.

- [2023] ShareGPT4V_ Improving large multi-modal models with better captions
- https://sharegpt4v.github.io/
- https://github.com/ShareGPT4Omni/ShareGPT4V/
- https://github.com/ShareGPT4Omni/ShareGPT4V/blob/master/docs/Data.md

## SA1B-Dense-Caption
---
- https://www.modelscope.cn/datasets/Tongyi-DataEngine/SA1B-Dense-Caption

## SA1B-Paired-Captions-Images
---
- https://www.modelscope.cn/datasets/Tongyi-DataEngine/SA1B-Paired-Captions-Images

## [2024 ECCV] DOCCI, Descriptions of Connected and Contrasting Images
---
- [2024 ECCV] DOCCI_ Descriptions of Connected and Contrasting Images
