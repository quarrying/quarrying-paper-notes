# Text

## [1994] MNIST
---
**References**:
- http://yann.lecun.com/exdb/mnist/


## [2010 ECCV] SVT, Street View Text
---
**References**:
- http://vision.ucsd.edu/~kai/svt/
- [2010 ECCV] Word Spotting in the Wild
- [2011 ICCV] End-to-end Scene Text Recognition


## [2011 NIPSW] SVHN, Street View House Numbers
---
> SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. 

There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training. 

**References**:
- http://ufldl.stanford.edu/housenumbers/
- [2011 NIPSW] Reading Digits in Natural Images with Unsupervised Feature Learning

## [2014 NIPS] MJSynth
---
合成数据集. MJ 是作者 Max Jaderberg 的首字母缩写.

> This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in our work.

- [2014 NIPS] Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition
- https://www.robots.ox.ac.uk/~vgg/data/text/

## [2013] CISIA-HWDB, HWDB1.0
---
**References**:
- http://www.nlpr.ia.ac.cn/databases/handwriting/download.html


## [2009] HCL2000
---
- [2009] HCL2000 - A Large-scale Handwritten Chinese Character Database for Handwritten Character Recognition


## [2009] Chars74K
---
**References**:
- http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/
- [2009] Character recognition in natural images


## [2003 ICDAR] ICDAR 2003
---
**References**:
- [2003 ICDAR] ICDAR 2003 robust reading competitions


## [2011] ICDAR 2011
---
The ICDAR 2011 dataset includes 229 and 255 images for training and testing.

**References**:
- Snoopertext: A multiresolution system for text detection in complex visual scenes


## [2013] ICDAR 2013
---
> The ICDAR 2013 dataset consists of 229 training images and 233 testing images in different resolutions. This dataset contains only horizontal or nearly horizontal text.

**References**:
- ICDAR 2013 robust reading competition (2013)


## [2015] ICDAR 2015
---
> ICDAR2015 contains natural images that are captured by Google Glasses casually, and most of them are severely distorted or blurred. There are 1000 training images and 500 testing images, which are annotated with quadrilaterals.

**References**:
- ICDAR 2015 Competition on robust reading (2015)
- ICDAR 2015 Competition Challenge 4: Incidental Scene Text

## [2016] IIIT-HWS
---
> IIIT-HWS dataset comprising of nearly 9M synthetic word images rendered out of 750 publicly available handwritten fonts. We use 90K unique words as the vocabulary which is picked from a popular open source English dictionary Hunspell.

- [2016] Generating Synthetic Data for Text Recognition


## [2017] ICDAR 2017
---
**References**:
- http://u-pat.org/ICDAR2017/index.php
- http://u-pat.org/ICDAR2017/program_competitions.php

### ICDAR2017-RCTW, RCTW-17, ICDAR2017 Competition on Reading Chinese Text in the Wild
共包含 12,000+ 图像, 大部分图片是通过手机摄像头在野外采集的. 有些是截图. 这些图片展示了各种各样的场景, 包括街景, 海报, 菜单, 室内场景和手机应用程序的截图. 

**References**:
- http://rctw.vlrlab.net/result/
- [2017] Icdar2017 competition on reading chinese text in the wild (rctw-17)

### ICDAR2017 Robust Reading Challenge on COCO-Text
**References**:
- https://rrc.cvc.uab.es/?ch=5&com=introduction
- [2017] ICDAR2017 Robust Reading Challenge on COCO-Text

### ICDAR2017-MLT, MLT-2017
> ICDAR 2017 MLT (IC17-MLT) is a large scale multi-lingual text dataset, which includes 7200 training images, 1800 validation images and 9000 testing images. The dataset is composed of complete scene images which come from 9 languages. Similarly with ICDAR 2015, the text regions in ICDAR 2017 MLT are also annotated by 4 vertices of the quadrangle.

> MLT-2017 dataset is a multi-language dataset. It includes 9 languages representing 6 different scripts. There are 7,200 training images, 1,800 validation images and 9,000 testing images in this dataset.

**References**:
- [2017] ICDAR2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt
- https://rrc.cvc.uab.es/?ch=8

## [2019] ICDAR 2019
---
**References**:
- http://icdar2019.org/

### ICDAR2019-LSVT, ICDAR 2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling
共 45w 中文街景图像, 包含 5w (2w 测试 + 3w 训练) 全标注数据 (文本坐标 + 文本内容), 40w 弱标注数据 (仅文本内容). 

**References**:
- https://ai.baidu.com/broad/download?dataset=lsvt

### ICDAR2019-ArT, ICDAR 2019 Robust Reading Challenge on Arbitrary-Shaped Text
共包含 10,166 张图像, 训练集 5603 图, 测试集 4563 图. 由 Total-Text, SCUT-CTW1500, Baidu Curved Scene Text 三部分组成, 包含水平, 多方向和弯曲等多种形状的文本. 

**References**:
- https://ai.baidu.com/broad/download?dataset=art

### ICDAR2019-ReCTS, ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard
> We collect and construct a practical and challenging multi-orientation natural scene text dataset (ReCTS) with 25,000 images, which consist of lots of signboards. In the dataset, all text lines and characters are labeled with locations and character codes. Besides, four tasks are presents for this competition: (1) end-to-end recognition on the signboards, (2) text line localization on the signboards, (3) character recognition on the signboards, (4) text line recognition on the signboards.

**References**:
- https://rrc.cvc.uab.es/?ch=12

### ICDAR2019-SROIE, Robust Reading Challenge on Scanned Receipts OCR and Information Extraction 2019
---

**References**:
- https://rrc.cvc.uab.es/?ch=13


## [2021] ICDAR 2021
---
**References**:
- https://icdar2021.org/


## [2019] DDI-100
---
**References**:
- [2019] DDI-100_ Dataset for Text Detection and Recognition


## [2017] CTW-1500, SCUT-CTW1500
---
> CTW1500 is a curved English text dataset that consists of 1000 training images and 500 testing images. All the text instances are annotated with 14 vertices.

**References**:
- [2017] Detecting curve text in the wild_ New dataset and new solution
- https://github.com/Yuliang-Liu/Curve-Text-Detector


## [2017] Total-Text
---
> Total-Text is a word-level based English text dataset. It consists of 1255 training images and 300 testing images, which contain horizontal texts, multi-oriented texts, and curved texts.

**References**:
- [2017 ICDAR] Total-text_ A comprehensive dataset for scene text detection and recognition
- [2020 IJDAR] Total-Text: toward orientation robustness in scene text detection
- https://github.com/cs-chan/Total-Text-Dataset


## [2016 CVPR] SynthText
---
> The SynthText dataset contains 800k synthesized text images, created via blending rendered words with natural images. As the location and transform of text are carefully chosen with a learning algorithm, the synthesized images look realistic.

> The dataset consists of 800 thousand images with approximately 8 million synthetic word instances. Each text instance is annotated with its text-string, word-level and character-level bounding-boxes.

该数据集是合成的, 但量较大, 常用作预训练.

**References**:
- [2016 CVPR] Synthetic data for text localisation in natural images
- https://www.robots.ox.ac.uk/~vgg/data/scenetext/


## [2016] COCO-Text
---
> It reuses the images from MS-COCO dataset.  Word regions are annotated in the form of axis-aligned bounding box (AABB)
> 
> The COCO-Text dataset is currently the largest dataset for scene text detection and recognition. It contains 43686 training images and 20000 images for validation/testing.

**References**:
- [2016] Coco-text_ Dataset and benchmark for text detection and recognition in natural images


## [2014] CUTE80
---
CUTE80: Curve Text Dataset

**References**:
- http://cs-chan.com/downloads_CUTE80_dataset.html
- [2014] A Robust Arbitrary Text Detection System for Natural Scene Images


## [2012 CVPR] MSRA-TD500, MSRA Text Detection 500
---
>The MSRA Text Detection 500 Database (MSRA-TD500) contains 500 natural images, which are taken from indoor (office and mall) and outdoor (street) scenes using a pocket camera. MSRA-TD500 is a multi-lingual long text dataset for Chinese and English. It includes 300 training images and 200 testing images with arbitrary orientations.

该数据集的标注级别是: text-line.

**References**:
- [2012 CVPR] Detecting texts of arbitrary orientations in natural images
- [2018 ACM MM] Arbitrary-Oriented Scene Text Detection via Rotation Proposals
- http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_%28MSRA-TD500%29


## [2014 TIP] HUST-TR400
---
**References**:
- [2014 TIP] A unified framework for multi-oriented text detection and recognition


## [2019] CTW
---
- 32,285 high resolution images
- 1,018,402 character instances
- 3,850 character categories
- 6 kinds of attributes

6 种属性为 occluded, bgcomplex, distorted, raised, wordart, handwritten

**References**:
- [2019] A Large Chinese Text Dataset in the Wild
- https://ctwdataset.github.io/
- https://github.com/yuantailing/ctw-baseline


## [2021] Chinese Scene Dataset
---
- [2021] Benchmarking chinese text recognition: Datasets, baselines, and an empirical study


## [2023] Union14M
---
> Union14M is a large scene text recognition (STR) dataset collected from 17 publicly available datasets, which contains 4M of labeled data (Union14M-L) and 10M of unlabeled data (Union14M-U)

- https://github.com/Mountchicken/Union14M

## 其他: 中文街景文字识别
---
共包括29万张图片, 其中21万张图片作为训练集 (带标注) , 8万张作为测试集 (无标注) . 数据集采自中国街景, 并由街景图片中的文字行区域 (例如店铺标牌, 地标等等) 截取出来而形成. 所有图像都经过一些预处理, 将文字区域利用仿射变化, 等比映射为一张高为48像素的图片. 

**References**:
- https://aistudio.baidu.com/aistudio/competition/detail/8


## 其他: 中文文档文字识别
---
共约364万张图片, 按照99:1划分成训练集和验证集. 数据利用中文语料库 (新闻 + 文言文) , 通过字体, 大小, 灰度, 模糊, 透视, 拉伸等变化随机生成.

**References**:
- https://github.com/YCG09/chinese_ocr


# Formula

## latex-formulas
----
> There are two datasets: raw_formulas and cleaned_formulas (This dataset has 550K formula-image pairs).
> 
> We scraped approximately 1 million LaTeX formula image-text pairs from arxiv that were uncleaned and without text segmentation to create the raw_formulas dataset. After cleaning the raw_formulas dataset and integrating it with the im2latex-100K dataset, we obtained the cleaned_formulas dataset, which has 550K formula-image pairs.

- https://huggingface.co/datasets/OleehyO/latex-formulas


## im2latex-100K
---
- https://github.com/Miffyli/im2latex-dataset
- [2017 ICML] Image-to-Markup Generation with Coarse-to-Fine Attention


# Table

## [2021 ICCV] WTW-Dataset
---
> WTW-Dataset is the first wild table dataset for table detection and table structure recongnition tasks, which is constructed from photoing, scanning and web pages, covers 7 challenging cases like: (1)Inclined tables, (2) Curved tables, (3) Occluded tables or blurredtables (4) Extreme aspect ratio tables (5) Overlaid tables, (6) Multi-color tables and (7) Irregular tables in table structure recognition.

- https://github.com/wangwen-whu/WTW-Dataset
- [2021 ICCV] Parsing Table Structures in the Wild

## [2020] PubTabNet
---
- [2020] Image-based table recognition: Data, model, and evaluation

## [2021] FinTabNet
---
- [2021] Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context

## [2022 CVPR] PubTables-1M
----
- [2022 CVPR] PubTables-1M: Towards comprehensive table extraction from unstructured documents


# Layout

## PubLayNet
----
训练集合中包含 35 万张图像，验证集合中包含 1.1 万张图像。总共包含 5 个类别，分别是： text, title, list, table, figure

- https://github.com/ibm-aur-nlp/PubLayNet

## CDLA
---
训练集合中包含 5000 张图像，验证集合中包含 1000 张图像。总共包含 10 个类别，分别是： Text, Title, Figure, Figure caption, Table, Table caption, Header, Footer, Reference, Equation

- https://github.com/buptlihang/CDLA


# KIE

## FUNSD
---
- https://guillaumejaume.github.io/FUNSD/

## XFUND
---
- https://github.com/doc-analysis/XFUND

## WildReceipt
---
- [2021] Spatial Dual-Modality Graph Reasoning for Key Information Extraction


# Document Parsing

## [2025] OmniDocBench
---
- https://opendatalab.com/OpenDataLab/OmniDocBench
- [2025] OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations

## [2025] MonkeyDoc 
---
> MonkeyDoc (the most comprehensive document parsing dataset to date), with 3.9 million instances spanning over ten document types in both Chinese and English. 

- [2025] MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm

