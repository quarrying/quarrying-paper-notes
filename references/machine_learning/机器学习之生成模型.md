# GAN, Generative Adversarial Nets

## GAN
----
开山之作, 第一作者是 Ian Goodfellow.

- [2014] Generative Adversarial Nets

## cGAN, Conditional GAN
---
- [2014] Conditional Generative Adversarial Nets

## DCGAN, Deep Convolutional GAN
---

- [2016 ICLR] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
- https://github.com/carpedm20/DCGAN-tensorflow
- https://github.com/Newmu/dcgan_code

## DiscoGAN
---
- [2017 ICML] Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

## WGAN, Wasserstein GAN
---
- [2017 ICML] Wasserstein generative adversarial networks

## WGAN-GP, WGAN-Gradient penalty
---
- [2017 NIPS] Improved Training of Wasserstein GANs

## LSGAN, Least Squares GAN
---
- [2017] Least Squares Generative Adversarial Networks

## SNGAN, Spectral Normalization GAN
--- 
- [2018 ICLR] Spectral normalization for generative adversarial networks

## BEGAN, Boundary Equilibrium GAN
---
- [2017] BEGAN_ Boundary Equilibrium Generative Adversarial Networks

## PG-GAN, PGGAN, ProGAN, Progressive GAN
---
- [2018 ICLR] Progressive Growing of GANs for Improved Quality, Stability, and Variation
- https://github.com/nashory/pggan-pytorch

## Fictitious GAN
---
- [2018] Fictitious GAN_ Training GANs with Historical Models

## TequilaGAN
---
- [2018] TequilaGAN_ How to easily identify GAN samples

## SAGAN
---
- [2018] Self-Attention Generative Adversarial Networks

## BigGAN
---
- [2018 ICLR] Large scale gan training for high fidelity natural image synthesis

## PCGAN
---
- [2019 AAAI] Pcgan_ Partition-controlled human image generation

## StyleGAN
---
!TODO: truncation trick

一些标记:
- input latent space: $\mathcal{Z}$
- native latent space: $\mathcal{W}$
- extended latent space: $\mathcal{W}+$

**References**
- [2019 CVPR] A style-based generator architecture for generative adversarial networks

## StyleGAN2
----
- [2020 CVPR] Analyzing and improving the image quality of stylegan

## StyleGAN2-ada
----
StyleGAN2-ada 提出了 adaptive discriminator augmentation (ADA), 可以将所需训练图像数减少至数千张.

> METFACES is our new dataset of 1336 high-quality faces extracted from the collection of Metropolitan Museum of Art (https://metmuseum.github.io/).

- [2020 NeurIPS] Training generative adversarial networks with limited data
- https://github.com/NVlabs/stylegan2-ada
- https://github.com/NVlabs/stylegan2-ada-pytorch

## InterFaceGAN 
---
> We therefore make an assumption that for any binary semantic (e.g., male v.s. female), there exists a hyperplane in the latent space serving as the separation boundary. Semantic remains the same when the latent code walks within one side of the hyperplane yet turns into the opposite when across the boundary.

- [2019] Interpreting the Latent Space of GANs for Semantic Face Editing
- https://github.com/a312863063/generators-with-stylegan2

## [2022 WACV] Latent to Latent_ A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-generated Images
---
该论文提出的方法与 InterFaceGAN, GANSpace 和 StyleFlow 是一类方法.

论文方法用到了多个预训练模型: 人脸识别模型, 人脸属性分类模型和 StyleGAN-v2. 这些预训练模型的参数在训练中是冻结的.

## Others
---
- [2016 NIPS] Improved Techniques for Training GANs
- [2017 ICLR] Adversarial Feature Learning
- [2018] The GAN Landscape_ Losses, Architectures, Regularization, and Normalization
- [2020] LSUN-Stanford Car Dataset_ Enhancing Large-Scale Car Image Datasets Using Deep Learning for Usage in GAN Training


# GAN Inversion

GAN Inversion 由 `[2016 ECCV] Generative visual manipulation on the natural image manifold` 提出. In this task, the latent vector from which a pretrained GAN most accurately reconstructs a given, known image, is sought.

## [2021] GAN inversion: A survey
---
关于 GAN inversion 的第一篇综述文献. 

GAN inversion 的问题定义:
$$ \mathbf{z}^* = \argmin_\mathbf{z} \ell\left( G(\mathbf{z}), x\right) \tag{1}$$ 

式中, $\ell\left( \cdot \right)$ 表示图像域或特征域上的损失函数 (论文中称 distance metric, 是等价的概念), 常用的有 $\ell_1$, $\ell_2$, perceptual 和 LPIPS; $G$ 是前馈神经网络模型, 在求解中其参数是冻结的. 

方程 (1) 有三大类解法: 1) learning-based; 2) optimization-based; 3) hybrid. 基于学习的方法速度快, 但重建误差大; 基于优化的方法重建精度高, 但速度慢. 

**References:**
- https://github.com/weihaox/awesome-gan-inversion

## [2016] IcGAN
---
- [2016] Invertible conditional gans for image editing

## [2017] AEGAN
---
- [2017] Learning inverse mapping by autoencoder based generative adversarial nets

## [2020 ECCV] IDInvert, In-domain invert encoding and inversion
---
- [2020 ECCV] In-domain gan inversion for real image editing

## [2019 ICCV] Image2StyleGAN
----
- [2019 ICCV] Image2stylegan_ How to embed images into the stylegan latent space?

## [2019] StyleGAN2 projection
---
- [2019] Analyzing and improving the image quality of StyleGAN

## [2020 CVPR] ALAE
---
- [2020 CVPR] Adversarial latent autoencoders

## PTI, Pivotal Tuning Inversion
---
本文提出的 Inversion 方法是一种基于优化的方法.

- [2021] Pivotal Tuning for Latent-based Editing of Real Images


# Diffusion Models, DM, 扩散模型

## [2021 ICLR] DDIM
----
- [2021 ICLR] Denoising diffusion implicit models

## [2021] LSGM
----
- [2021] Score-based generative modeling in latent space

## [2021] ADM
----
- [2021] Diffusion models beat gans on image synthesis

## [2020 NIPS] Denoising diffusion probabilistic models
----

## [2022 CVPR] StableDiffusion, Stable Diffusion, LDM, latent diffusion model
----
> In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions.

与 latent diffusion model 相对的是 pixel-based diffusion model.

- [2022 CVPR] High-resolution image synthesis with latent diffusion models
- https://github.com/CompVis/latent-diffusion


## [2015] Deep unsupervised learning using nonequilibrium thermodynamics
----


# Variational autoencoder, VAE

## [2014 ICLR] Auto-Encoding Variational Bayes
----

## [2020] Very deep vaes generalize autoregressive models and can outperform them on images
----

## [2020 NeurIPS] NVAE: A deep hierarchical variational autoencoder
----


# Flow-based models

## [2015] Nice: Non-linear independent components estimation
----

## [2017 ICLR] Density estimation using real NVP
----

## [2018 NIPS] Glow: Generative flow with invertible 1x1 convolutions
----

# Autoregressive models, ARM, 自回归模型

## [2020 ICML] Generative pretraining from pixels
----

## [2019] Generating long sequences with sparse transformers
----

## [2016 NIPS] Conditional image generation with pixelcnn decoders
----

## [2016] Pixel recurrent neural networks
----


# 一些链接
- https://github.com/hindupuravinash/the-gan-zoo
- https://github.com/zhangqianhui/AdversarialNetsPapers
- https://github.com/nightrome/really-awesome-gan
- https://github.com/nashory/gans-awesome-applications
- thisxdoesnotexist
    - https://thisxdoesnotexist.com/
    - https://thisvesseldoesnotexist.com/
    - https://thispersondoesnotexist.com/
    - https://thiscatdoesnotexist.com/
