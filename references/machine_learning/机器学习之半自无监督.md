## *Un-/Self-supervised representation learning*
---
### [2018 ECCV] Deep clustering for unsupervised learning of visual features
---

### MoCo v1
---
- [2019] Momentum Contrast for Unsupervised Visual Representation Learning
- https://github.com/facebookresearch/moco

### MoCo v2
---
- [2020] Improved baselines with momentum contrastive learning
- https://github.com/facebookresearch/moco

### SimCLR
---
SimCLR 对 batch size 的要求的表较大.

NT-Xent (the normalized temperature-scaled cross entropy loss):
$$\ell_{i,j} = -\log \frac{\exp(\mathrm{sim}(\bm z_i, \bm z_j)/\tau)}{\sum_{k=1}^{2N} \mathbbm{k \neq i}\exp(\mathrm{sim}(\bm z_i, \bm z_k)/\tau)}$$
只在正例对上计算该损失项.

Pretext task, 或称 surrogate task. 常见的有:
- relative patch prediction
- solving jigsaw puzzles
- colorization
- rotation prediction

问题: 在用 ImageNet 做自监督训练和评估时
1) linear evaluation: 自监督训练的训练集, linear classifier 的训练集和测试集?
2) fine-tune: 自监督训练的训练集, fine-tune 的训练集和测试集?

**References**:
- [2020] A simple framework for contrastive learning of visual representations
- https://github.com/google-research/simclr
- https://github.com/sthalles/SimCLR

### SimCLR v2
---
- [2020] Big Self-Supervised Models are Strong Semi-Supervised Learners

### BYOL
---
- [2020] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning

### SwAV, Swapping Assignments between Views
----
> SwAV [7] incorporates clustering into a Siamese network, by computing the assignment from one view and predicting it from another view. SwAV performs online clustering under a balanced partition constraint for each batch, which is solved by the Sinkhorn-Knopp transform [10].
>> [2021 CVPR] Exploring Simple Siamese Representation Learning


- [2020 NeurIPS] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

### SimSiam, Simple Siamese
---
SimSiam 可以视作 BYOL without the momentum encoder; SimCLR without negative pairs; SwAV without online clustering. 

$x$ 经过不同的数据增强方式得到 $x_1$ 和 $x_2$, 编码网络记为 $f$, prediction MLP head 记为 $h$. 定义:

$x_1$                      | $x_2$
---------------------------|---------
$z_1 \triangleq f(x_1)$    | $z_2 \triangleq f(x_2)$
$p_1 \triangleq h(f(x_1))$ | $p_2 \triangleq h(f(x_2))$

另外, 定义: 
$$\mathcal{D}(p_1, z_2) = -\frac{p_1}{\lVert p_1 \rVert_2}\frac{z_2}{\lVert z_2 \rVert_2}$$
$$\mathcal{D}(p_2, z_1) = -\frac{p_2}{\lVert p_2 \rVert_2}\frac{z_1}{\lVert z_1 \rVert_2}$$

则最终损失函数为: 
$$\mathcal{L} = \frac{1}{2}\mathcal{D}(p_1, z_2) + \frac{1}{2}\mathcal{D}(p_2, z_1)$$

**References**:
- [2021 CVPR] Exploring Simple Siamese Representation Learning

### [2020 CVPR] How Useful is Self-Supervised Pretraining for Visual Task
---

### [2020] Rethinking Image Mixture for Unsupervised Visual Representation Learning
---

### [2020] What Makes for Good Views for Contrastive Learning
---

### [2018] Unsupervised representation learning by predicting image rotations
----

## *Semi-Supervised*
---
### MixMatch
---
- [2019] MixMatch: A Holistic Approach to Semi-Supervised Learning
### ReMixMatch 
---
- [2019] ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring

### FixMatch
---
- [2020] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence

## *Pretrain and Self-training*
----

### Rethink Pretrain and Self-training
---