- MoCo v1
    - [2019] Momentum Contrast for Unsupervised Visual Representation Learning
    - https://github.com/facebookresearch/moco
- MoCo v2
    - [2020] Improved baselines with momentum contrastive learning
    - https://github.com/facebookresearch/moco
- SimCLR
    - [2020] A simple framework for contrastive learning of visual representations
    - https://github.com/google-research/simclr
    - https://github.com/sthalles/SimCLR
- SimCLR v2
    - [2020] Big Self-Supervised Models are Strong Semi-Supervised Learners
- BYOL
    - [2020] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
- SwAV, Swapping Assignments between Views
    - [2020 NeurIPS] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments
- How Useful is Self-Supervised Pretraining for Visual Task
- Rethinking Image Mixture for Unsupervised Visual Representation Learning
- Rethink Pretrain and Self-training
- What Makes for Good Views for Contrastive Learning
- mixmatch
- remixmatch
- fixmatch

