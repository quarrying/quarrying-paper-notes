# *Un-/Self-supervised representation learning*
---

## Exemplar CNN
---
- [2014 NIPS] Discriminative unsupervised feature learning with convolutional neural networks

## [2018 ECCV] Deep clustering for unsupervised learning of visual features
---

## [2018] Unsupervised Feature Learning via Non-Parametric Instance Discrimination
---
> Our novel unsupervised feature learning approach is instance-level discrimination. We treat each image instance as a distinct class of its own and train a classifier to distinguish between individual instance classes.

## MoCo v1
---
- [2019] Momentum Contrast for Unsupervised Visual Representation Learning
- https://github.com/facebookresearch/moco

## MoCo v2
---
- [2020] Improved baselines with momentum contrastive learning
- https://github.com/facebookresearch/moco

## SimCLR
---
SimCLR 对 batch size 的要求的比较大.

NT-Xent (the normalized temperature-scaled cross entropy loss):
$$\ell_{i,j} = -\log \frac{\exp(\mathrm{sim}(\bm z_i, \bm z_j)/\tau)}{\sum_{k=1}^{2N} \mathbbm{k \neq i}\exp(\mathrm{sim}(\bm z_i, \bm z_k)/\tau)}$$
只在正例对上计算该损失项.

**References**:
- [2020] A simple framework for contrastive learning of visual representations
- https://github.com/google-research/simclr
- https://github.com/sthalles/SimCLR

## SimCLR v2
---
- [2020] Big Self-Supervised Models are Strong Semi-Supervised Learners

## BYOL
---
- [2020] Bootstrap Your Own Latent A New Approach to Self-Supervised Learning

## SwAV, Swapping Assignments between Views
----
> SwAV [7] incorporates clustering into a Siamese network, by computing the assignment from one view and predicting it from another view. SwAV performs online clustering under a balanced partition constraint for each batch, which is solved by the Sinkhorn-Knopp transform [10].
>> [2021 CVPR] Exploring Simple Siamese Representation Learning


- [2020 NeurIPS] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

## SimSiam, Simple Siamese
---
SimSiam 可以视作 BYOL without the momentum encoder; SimCLR without negative pairs; SwAV without online clustering. 

$x$ 经过不同的数据增强方式得到 $x_1$ 和 $x_2$, 编码网络记为 $f$, prediction MLP head 记为 $h$. 定义:

$x_1$                      | $x_2$
---------------------------|---------
$z_1 \triangleq f(x_1)$    | $z_2 \triangleq f(x_2)$
$p_1 \triangleq h(f(x_1))$ | $p_2 \triangleq h(f(x_2))$

另外, 定义: 
$$\mathcal{D}(p_1, z_2) = -\frac{p_1}{\lVert p_1 \rVert_2}\frac{z_2}{\lVert z_2 \rVert_2}$$
$$\mathcal{D}(p_2, z_1) = -\frac{p_2}{\lVert p_2 \rVert_2}\frac{z_1}{\lVert z_1 \rVert_2}$$

则最终损失函数为: 
$$\mathcal{L} = \frac{1}{2}\mathcal{D}(p_1, z_2) + \frac{1}{2}\mathcal{D}(p_2, z_1)$$

由损失函数可见, SimSiam 没有反例对 (positive sample pair).

### 源码阅读
阅读 SimSiam 的官方源码 (https://github.com/facebookresearch/simsiam), 遇到一个困惑:
由 `main_simsiam.py` 的 argparse argument 相关代码可见, 
- world_size 是 number of nodes for distributed training
- rank 是 node rank for distributed training

这两个概念具有误导性, 因为通常情况下, world_size 指的是所有节点的 gpu 的总数, rank 是 gpu 的 rank. 通读代码后, 发现 main_simsiam.py 后面对 argument 中的 world_size 和 rank 进行了调整, 调整后的意义与通常情况下的意义一致, 相关代码如下, 困惑解除.

```python
# Since we have ngpus_per_node processes per node, the total world_size
# needs to be adjusted accordingly
args.world_size = ngpus_per_node * args.world_size
# For multiprocessing distributed training, rank needs to be the
# global rank among all the processes
args.rank = args.rank * ngpus_per_node + gpu
```

**References**:
- [2021 CVPR] Exploring Simple Siamese Representation Learning
- https://github.com/facebookresearch/simsiam

## [2020 CVPR] How Useful is Self-Supervised Pretraining for Visual Task
---

## [2020] Rethinking Image Mixture for Unsupervised Visual Representation Learning
---

## [2020] What Makes for Good Views for Contrastive Learning
---

## [2018] Unsupervised representation learning by predicting image rotations
----

## [2021] SimMIM_ A Simple Framework for Masked Image Modeling
---

# *Semi-Supervised*
---
## MixMatch
---
- [2019] MixMatch: A Holistic Approach to Semi-Supervised Learning

## ReMixMatch 
---
- [2019] ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring

## FixMatch
---
- [2020] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence

# *Pretrain and Self-training*
----

## Rethink Pretrain and Self-training
---