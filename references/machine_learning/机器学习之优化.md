**OPINIONS**:
1) 个人总结, 学习率调整可以分为如下方式:
- 基于策略的学习率调整, 如: cosine learning rate decay
- 基于统计的学习率调整, 如: ADAM
- 基于梯度的学习率调整, 如: hypergradient descent

## cosine learning rate decay
---
- [2017 ICLR] SGDR_ Stochastic gradient descent with warm restarts
- [2019 CVPR] Bag of tricks for image classification with convolutional neural networks

## LARS, Layer-wise Adaptive Rate Scaling
---
当 batch size 很大时, 采用传统方法 (linear learning rate scaling with warm-up) 训练的模型, 精度会下降. 于是提出了 LARS ( Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.)

> There are two notable differences between LARS and other adaptive algorithms such as ADAM (Kingma &
> Ba (2014)) or RMSProp (Tieleman & Hinton (2012)): first, LARS uses a separate learning rate for
> each layer and not for each weight, which leads to better stability. And second, the magnitude of the
> update is controlled with respect to the weight norm for better control of training speed.

- [2017] Large batch training of convolutional networks

## linear learning rate scaling 
---
batch size 增加 k 倍, 则 LR 相应的增加 k 倍, 同时保持其他超参 (如 momentum, weight decay 等) 不变.

> It was observed that linear scaling works much better for networks with Batch
> Normalization (e.g. Codreanu et al. (2017)).

- [2014] One weird trick for parallelizing convolutional neural networks

## warm-up
---
- [2017] Accurate, large minibatch sgd: Training imagenet in 1 hour

## STLR, Slanted Triangular Learning Rates
---
Slanted Triangular Learning Rates (STLR) is a learning rate schedule which first linearly increases the learning rate and then linearly decays it, which can be seen in Figure to the right. It is a modification of Triangular Learning Rates, with a short increase and a long decay period.

- [2018 ACL] Universal Language Model Fine-tuning for Text Classification


## Hypergradient Descent
--- 
- [2017] Online Learning Rate Adaptation with Hypergradient Descent
