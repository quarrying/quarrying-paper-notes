# CNN
----

## LeNet-5
---
- [1998 IEEE] Gradient-based learning applied to document recognition

## AlexNet
---
- [2012 NIPS] ImageNet Classification with Deep Convolutional Neural Networks

## NIN
----
最早提出 `1*1` 卷积.

- [2013] Network in network

## ZFNet
---
- [2014 ECCV] Visualizing and understanding convolutional networks

## VGG
---
VGG 是 Visual Geometry Group 的简称. 常见的有: VGG16 和 VGG19, 其中数字指的是卷积层和全连接层的数目.

> So what have we gained by using, for instance, a stack of three 3*3 conv. layers instead of a single 7*7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3*3 convolution stack has C channels, the stack is parametrised by 3*3^2*C^2 = 27*C^2 weights; at the same time, a single 7*7 conv. layer would require 7^2*C^2 = 49*C^2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7*7 conv. filters, forcing them to have a decomposition through the 3*3 filters (with non-linearity injected in between).

- [2014] Very Deep Convolutional Networks for Large-Scale Image Recognition

## Inception
---
- [2015 CVPR] Going deeper with convolutions
- [2015 ICML] Batch normalization: Accelerating deep network training by reducing internal covariate shift
- [2015] Rethinking the inception architecture for computer vision
- [2016] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
- [2017 CVPR] Xception_ Deep Learning with Depthwise Separable Convolutions

## ResNet
---
常见的有: ResNet 18/34/50/101/152/200/1001, 其中数字指的是卷积层和全连接层的数目.

模型        | 预训练模型文件大小
------------|----------------
ResNet18    | 44.66MB
ResNet34    | 83.27MB
ResNet50    | 97.78MB
ResNet101   | 170.51MB
ResNet152   | 230.43MB
MobileNetV2 | 13.55MB

参数量可以用 **预训练模型文件大小 / 4** 来估算 (2^10 约等于 1000, 模型文件中权重是单精度浮点型的, 有 4 字节).

**References**
- [2016 CVPR] Deep residual learning for image recognition
- [2016 ECCV] Identity mappings in deep residual networks
- [2016] Aggregated Residual Transformations for Deep Neural Networks
- [2016 BMVC] Wide Residual Networks

## DenseNet
---
为 ImageNet 设计的 DenseNet (参考文献中的 Table 1), 前两层为: `7 × 7 conv, stride 2` 和 `3 × 3 max pool, stride 2`. 在 [2018 CVPR] Scale-Transferrable Object Detection 中提到这种设计并不好, 并用三个连续的卷积替换之:

> Inspired by DSOD [27], we replace the input layers (7 × 7 convolution layer, stride = 2 followed by a 3 × 3 max pooling layer, stride = 2) into three 3×3 convolution layers and one 2×2 mean pooling layer. The stride of the first convolution layer is 2 and the others are 1. The output channels for all three convolution layers are 64. We call these layers “stem block”. Table 1 depicts our network architecture in detail. Experiments show that this simple substitution can significantly improve the accuracy of object detection (see Table 3 in ablation study). One explanation could be that the input layers in the original DenseNet-169 have lost much information due to two consecutive down sampling. This will impair the performance of object detection, especially for small objects.

- [2016] Densely Connected Convolutional Networks

## DPN
---
- [2017] Dual Path Networks

## GUNN
---
- [2017] Gradually Updated Neural Networks for Large-Scale Image Recognition

## SENet
---
- [2017] Squeeze-and-Excitation Networks

## DLA
---
- [2017] Deep Layer Aggregation

## BAM, CBAM
---
- [2018 BMVC] BAM_ Bottleneck Attention Module
- [2018 ECCV] CBAM_ Convolutional Block Attention Module

## IBN-Net
---
- [2018 ECCV] Two at Once_ Enhancing Learning and Generalization Capacities via IBN-Net

## SKNet
---
> SKNet convolves the feature map with 3 × 3 and 5 × 5 kernels respectively, and then obtains the weight of the two convolution results through GAP to do attention.

- [2019] Selective Kernel Networks

## Res2Net
---
- [2019] Res2Net_ A New Multi-scale Backbone Architecture

## GCNet
---
- [2019] GCNet_ Non-local Networks Meet Squeeze-Excitation Networks and Beyond

## ResNeSt
---
- [2020] ResNeSt_ Split-Attention Networks

## SGE-NET
----
- [2019] Spatial group-wise enhance: Improving semantic feature learning in convolutional networks


# 轻量型 CNN
---

## SqueezeNet
---
- [2016] SqueezeNet_ AlexNet-level accuracy with 50x fewer parameters and less than 0.5MB model size

## IGCV
---
- [2017 ICCV] Interleaved Group Convolutions for Deep Neural Networks
- [2018] IGCV2_ Interleaved Structured Sparse Convolutional Neural Networks
- [2018] IGCV3_ Interleaved low-rank group convolutions for efficient deep neural networks

## CondenseNet
---
- [2017] CondenseNet_ An Efficient DenseNet using Learned Group Convolutions

## SqueezeNext
---
- [2018] SqueezeNext_ Hardware-Aware Neural Network Design

## MobileNet
---
- [2017] MobileNets_ Efficient Convolutional Neural Networks for Mobile Vision Applications
- [2018] Inverted Residuals and Linear Bottlenecks_ Mobile Networks for Classification, Detection and Segmentation
- [2019] Searching for MobileNetV3

## ShuffleNet
---
- [2017] ShuffleNet_ An Extremely Efficient Convolutional Neural Network for Mobile Devices
- [2018] ShuffleNet V2_ Practical Guidelines for Efficient CNN Architecture Design

## MobileFaceNets
---
- [2018] MobileFaceNets_ Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices

## MnasNet
---
- [2018] MnasNet_ Platform-Aware Neural Architecture Search for Mobile

## ChannelNets
---
- [2018 NIPS] ChannelNets_ Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions

## MobileNeXt
---
- [[2020] MobileNeXt_ Rethinking Bottleneck Structure for Efficient Mobile Network Design](https://arxiv.org/abs/2007.02269)
- https://github.com/zhoudaquan/rethinking_bottleneck_design

## ConvNeXt
---
- [2022 CVPR] A ConvNet for the 2020s


# Block or Module
---

## Deformable convolution
----
- [2017] Deformable Convolutional Networks
- [2019 CVPR] Deformable convnets v2: More deformable, better results

## SlimConv
---
- [如何评价新型即插即用模块SlimConv？](https://www.zhihu.com/question/393850908 )
- [2020] SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks by Weights Flipping

## SKConv, Selective Kernel Convolution
---
- [2019 CVPR] Selective Kernel Networks

## MixConv
---
- [2019 BMVC] MixConv: Mixed Depthwise Convolutional Kernels

## OctConv, Octave Convolution
---
- [如何评价最新的Octave Convolution？](https://www.zhihu.com/question/320462422)
- [2019 ICCV] Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution


# Attention Module
----

## Overview
----
- 注意力分为: Hard-attention, Soft-attention, Self-attention
- 空间注意力, 通道注意力
- 注意力与显著性之间的区别与联系?

## Others
----
- SE
- CBAM
- [2017 CVPR] Residual Attention Network for Image Classification
- [2018] CCNet_ Criss-Cross Attention for Semantic Segmentation
- [2020] Attentional Feature Fusion
    - https://github.com/YimianDai/open-aff
- [2018 CVPR] Non-local neural networks
- DANet
    - [2019 CVPR] Dual attention network for scene segmentation
- [2019 ICCV] Attention augmented convolutional networks


# MLP
----

## MLP-Mixer
----
- [2021 NeurIPS] Mlp-mixer: An all-mlp architecture for vision
