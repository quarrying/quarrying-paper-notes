# Activation Function

## Leaky ReLU
---
- [2013 ICML] Rectifier nonlinearities improve neural network acoustic models

## Thresholded ReLU
---
- [2014] Zero-Bias Autoencoders and the Benefits of Co-Adapting Features

## PReLU, Parametric ReLU
---
- [2015] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

## ELU, Exponential Linear Unit
---
- [2015] Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)

## S-shaped ReLU
---
- [2015] Deep Learning with S-shaped Rectified Linear Activation Units

## MPELU, Multiple Parametric Exponential Linear Unit
---
- [2016 ECCV] Improving Deep Neural Network with Multiple Parametric Exponential Linear Units

## SELU, Self-Normalizing Linear Unit
---
- [2017] Self-Normalizing Neural Networks

## Swish
---
- [2017] Swish_ a Self-Gated Activation Function

## PoLU, Power Linear Unit
---
- [2018] Training Neural Networks by Using Power Linear Units (PoLUs)

## Mish
---
- [2019] Mish_ A self regularized non-monotonic neural activation function

## Squareplus: ReLU 的平滑近似
----
- [[2021] Squareplus: A Softplus-Like Algebraic Rectifier](https://arxiv.org/abs/2112.11687)

## [2015] Empirical Evaluation of Rectified Activations in Convolution Network
---

## [2016 ICML] Noisy Activation Functions
---

## [2016] Revise Saturated Activation Functions
---

## [2018] Activation Functions_ Comparison of trends in Practice and Research for Deep Learning
---

## [2018] Deep Neural Networks with Data Dependent Implicit Activation Function
---

## [2018] The Quest for the Golden Activation Function
---

