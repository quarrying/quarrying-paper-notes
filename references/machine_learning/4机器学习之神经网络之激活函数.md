# Activation
- Leaky ReLU
    - [2013 ICML] Rectifier nonlinearities improve neural network acoustic models
- Thresholded ReLU
    - [2014] Zero-Bias Autoencoders and the Benefits of Co-Adapting Features
- PReLU, Parametric ReLU
    - [2015] Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
- ELU
    - [2015] Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
- S-shaped ReLU
    - [2015] Deep Learning with S-shaped Rectified Linear Activation Units
- MPELU
    - [2016 ECCV] Improving Deep Neural Network with Multiple Parametric Exponential Linear Units
- SELU
    - [2017] Self-Normalizing Neural Networks
- Swish
    - [2017] Swish_ a Self-Gated Activation Function
- PoLU
    - [2018] Training Neural Networks by Using Power Linear Units (PoLUs)
- Others
    - [2015] Empirical Evaluation of Rectified Activations in Convolution Network
    - [2016 ICML] Noisy Activation Functions
    - [2016] Revise Saturated Activation Functions
    - [2018] Activation Functions_ Comparison of trends in Practice and Research for Deep Learning
    - [2018] Deep Neural Networks with Data Dependent Implicit Activation Function
    - [2018] The Quest for the Golden Activation Function
    
    