**OPINIONS**: 
- 数据重采样, 相当于数据加权. 
- 应该还包括学习率加权.
- 加权 (数据加权, 损失加权, 梯度加权, 学习率加权) 包括: 实例级别, 类级别.
- 加权可以分为软加权和硬加权.

# Data re-sampling
- [2002] Smote: synthetic minority over-sampling technique
- [2005] Borderline-smote: a new over-sampling method in imbalanced data sets learning
- [2016 CVPR] Learning deep representation for imbalanced classification
- [2019 NeurIPS] Learning imbalanced datasets with label-distribution-aware margin loss

## bootstrapping, hard negative mining
> bootstrapping (and now often called hard negative mining), has existed for at least 20 years. Bootstrapping was introduced in the work of Sung and Poggio [30] in the mid-1990’s (if not earlier) for training face detection models. Their key idea was to gradually grow, or bootstrap, the set of background examples by selecting those examples for which the detector triggers a false alarm. This strategy leads to an iterative training algorithm that alternates between updating the detection model given the current set of examples, and then using the updated model to find new false positives to add to the bootstrapped training set. The process typically commences with a training set consisting of all object examples and a small, random set of background examples.
>> [2016 CVPR] Training Region-based Object Detectors with Online Hard Example Mining

# Loss re-weighting
- [2017 ICCV] Focal loss for dense object detection

# Gradient re-weighting
- [2021 CVPR] Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection

# Feature augmentation
- [2019 CVPR] Large-scale long-tailed recognition in an open world
- [2020] Feature space augmentation for long-tailed data

# Decoupling training
- [2019] Decoupling representation and classifier for long-tailed recognition
