# Regularization
---

## Weight Decay, L2 Regularization
----
- [1991 NIPS] A simple weight decay can improve generalization

## Dropout
----
It was mostly applied on top layers that had a large number of parameters to prevent feature coadaptation and overfitting.

- [2013 NIPS] Dropout Training as Adaptive Regularization
- [2014 JMLR] Dropout_ A Simple Way to Prevent Neural Networks from Overfitting

## DropConnect
----
DropConnect 与 Dropout 之间的区别: DropConnect is very similar to dropout, except that it does not drop the output value, but instead the input value of some nodes.

- [2013 ICML] Regularization of Neural Networks using DropConnect

## LSR, Label Smoothing Regularization
---
- [2016 CVPR] Rethinking the Inception Architecture for Computer Vision
- [2017 ICLR] Regularizing Neural Networks By Penalizing Confident Output Distributions
- [2019] When Does Label Smoothing Help
- [202011] Delving Deep into Label Smoothing

## DisturbLabel
----
- [2016] DisturbLabel_ Regularizing CNN on the Loss Layer

## Noise
----
- [2017 NIPS] Regularizing Deep Neural Networks by Noise_ Its Interpretation and Optimization

## Feature Incay
---
- [2017] Feature Incay for Representation Regularization

## PatchShuffle
---
- [2017] PatchShuffle Regularization

## Shake-Shake
---
- [2017] Shake-Shake regularization

## SVB, Singular Value Bounding
----
- [2017 CVPR] Improving Training of Deep Neural Networks via Singular Value Bounding

## Shakeout
---
- [2017 TPAMI] Shakeout: A New Approach to Regularized Deep Neural Network Training

## VISER
---
- [2018] VISER_ Visual Self-Regularization

## ShakeDrop
---
- [2018] To realize a similar regularization to Shake-Shake on 1-branch network architectures

## Orthogonality Regularizations
---
- [2018] Can We Gain More from Orthogonality Regularizations in Training Deep CNNs

## DropBlock
---
- [2018 NIPS] DropBlock_ A regularization method for convolutional networks

## DropFilter
---
- [2018] DropFilter_ Dropout for Convolutions


# Normalization
---

## Batch Normalization
---
> BN uses mini-batch statistics during training and replace them with popular statistics during inference, introducing discrepancy between training and inference. Batch renormalization [21] was recently proposed to address this issue by gradually using popular statistics during training. As another interesting application of BN, Li et al. [34] found that BN can alleviate domain shifts by recomputing popular statistics in the target domain.
>> [2017 ICCV] Arbitrary style transfer in real-time with adaptive instance normalization

- [2015] Batch Normalization_ Accelerating Deep Network Training b y Reducing Internal Covariate Shift
- [2018] Batch Normalization and the impact of batch structure on the behavior of deep convolution networks
- [2018] Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift
- [2019] An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks
- [2018 NeurIPS] How Does Batch Normalization Help Optimization?

## Layer Normalization
----
- [2016] Layer Normalization

## Group Normalization
---
- [2018] Group Normalization

## Switchable Normalization
---
- [2018] Differentiable Learning-to-Normalize via Switchable Normalization


# Data Augmentation

## AutoAugment
---
- [2018] AutoAugment: Learning Augmentation Policies from Data

## mixup
---
- [2017] mixup_ Beyond Empirical Risk Minimization
- [2018] Improved Mixed-Example Data Augmentation

## SamplePairing
---
- [2018] Data Augmentation by Pairing Samples for Images Classification

## Unsupervised Data Augmentation
---
- [2019] Unsupervised Data Augmentation

## CutMix
---
- [2019 ICCV]  CutMix: Regularization strategy to train strong classifiers with localizable features

## Mosaic
---
- [2020] YOLOv4_ Optimal Speed and Accuracy of Object Detection

## Scale Jittering
---
>> [2015 ICLR] Very deep convolutional networks for large-scale image recognition
> 
> The second approach to setting is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [S min, S max ] (we used > S min = 256 and S max = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training > set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales.

>> [2015] Deep Residual Learning for Image Recognition
> 
> The image is resized with its shorter side randomly sampled in [256,480] for scale augmentation. A 224×224 crop is randomly sampled from an image or its horizontal flip.

>> [2014 arXiv] Going deeper with convolutions
> 
> Still, one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly > between 8% and 100% of the image area and whose aspect ratio is chosen randomly between 3/4 and 4/3.

# Data Augmentation using Information Dropping
---

## Random Erasing
---
- [2017] Random erasing data augmentation

## cutout
---
- [2017] Improved regularization of convolutional neural networks with cutout

## Hide-and-seek (HaS)
---
divide the picture evenly into small squares and delete them randomly.

- [2018] Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond

## GridMask
---
- [202001] GridMask Data Augmentation
- https://github.com/akuxcw/GridMask


# Others
---
- [2017] L2 Regularization versus Batch and Weight Normalization
- [2017] Regularizing neural networks by penalizing confident output distributions
- [2017] Dataset Augmentation in Feature Space
- [2018] Data augmentation instead of explicit regularization
- [2018] Do deep nets really need weight decay and dropout
- https://github.com/takmin/DataAugmentation
- https://github.com/codebox/image_augmentor
- https://github.com/albumentations-team/albumentations

