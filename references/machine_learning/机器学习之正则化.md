# Regularization
---

## Weight Decay, L2 Regularization
----
- [1991 NIPS] A simple weight decay can improve generalization

## Dropout
----
It was mostly applied on top layers that had a large number of parameters to prevent feature coadaptation and overfitting.

- [2013 NIPS] Dropout Training as Adaptive Regularization
- [2014 JMLR] Dropout_ A Simple Way to Prevent Neural Networks from Overfitting

## DropConnect
----
DropConnect 与 Dropout 之间的区别: DropConnect is very similar to dropout, except that it does not drop the output value, but instead the input value of some nodes.

- [2013 ICML] Regularization of Neural Networks using DropConnect

## LSR, Label Smoothing Regularization
---
- [2016 CVPR] Rethinking the Inception Architecture for Computer Vision
- [2017 ICLR] Regularizing Neural Networks By Penalizing Confident Output Distributions
- [2019] When Does Label Smoothing Help
- [202011] Delving Deep into Label Smoothing

## DisturbLabel
----
- [2016] DisturbLabel_ Regularizing CNN on the Loss Layer

## Noise
----
- [2017 NIPS] Regularizing Deep Neural Networks by Noise_ Its Interpretation and Optimization

## Feature Incay
---
- [2017] Feature Incay for Representation Regularization

## PatchShuffle
---
- [2017] PatchShuffle Regularization

## Shake-Shake
---
- [2017] Shake-Shake regularization

## SVB, Singular Value Bounding
----
- [2017 CVPR] Improving Training of Deep Neural Networks via Singular Value Bounding

## Shakeout
---
- [2017 TPAMI] Shakeout: A New Approach to Regularized Deep Neural Network Training

## VISER
---
- [2018] VISER_ Visual Self-Regularization

## ShakeDrop
---
- [2018] To realize a similar regularization to Shake-Shake on 1-branch network architectures

## Orthogonality Regularizations, 正交正则化
---
- [2018] Can We Gain More from Orthogonality Regularizations in Training Deep CNNs

## DropBlock
---
- [2018 NIPS] DropBlock_ A regularization method for convolutional networks

## DropFilter
---
- [2018] DropFilter_ Dropout for Convolutions


# Normalization
---

## Batch Normalization
---
> BN uses mini-batch statistics during training and replace them with popular statistics during inference, introducing discrepancy between training and inference. Batch renormalization [21] was recently proposed to address this issue by gradually using popular statistics during training. As another interesting application of BN, Li et al. [34] found that BN can alleviate domain shifts by recomputing popular statistics in the target domain.
>> [2017 ICCV] Arbitrary style transfer in real-time with adaptive instance normalization

- [2015] Batch Normalization_ Accelerating Deep Network Training b y Reducing Internal Covariate Shift
- [2018] Batch Normalization and the impact of batch structure on the behavior of deep convolution networks
- [2018] Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift
- [2019] An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks
- [2018 NeurIPS] How Does Batch Normalization Help Optimization?

## Layer Normalization
----
- [2016] Layer Normalization

## Group Normalization
---
- [2018] Group Normalization

## Switchable Normalization
---
- [2018] Differentiable Learning-to-Normalize via Switchable Normalization

## Weight normalization
---

$$\mathbf{w} = \frac{g}{||\mathbf{v}||} \mathbf{v}$$

### 损失项关于 $g$ 的微分
$$
\mathrm{d} L = \nabla_{\mathbf w}^TL \mathrm{d} \mathrm{w} = \frac{\nabla_{\mathbf w}^TL \mathbf{v}}{||\mathbf{v}||} \mathrm{d}g
$$
于是
$$
\nabla_g L = \frac{\nabla_{\mathbf w}^T L \mathbf{v}}{||\mathbf{v}||}
$$

### 损失项关于 $\mathbf{v}$ 的微分
$$
\mathrm{d} L = \nabla_{\mathbf w}^TL \mathrm{d} \mathrm{w} = g \nabla_{\mathbf w}^TL \mathrm{d}\frac{\mathbf{v}}{||\mathbf{v}||} 
$$

因为 
$$
\mathrm{d}\frac{\mathbf{v}}{||\mathbf{v}||} =  \frac{{{{|| {\mathbf{v}} ||}^2}I - \mathbf{v}{{\mathbf{v}}^T}}}{{{\left\| {\mathbf{v}} \right\|}^3}}\mathrm{d}\mathbf{v}
$$
所以
$$
\begin{aligned}
\mathrm{d} L 
&= g \nabla_{\mathbf w}^TL \frac{{{{|| {\mathbf{v}} ||}^2}I - \mathbf{v}{{\mathbf{v}}^T}}}{{{\left\| {\mathbf{v}} \right\|}^3}}\mathrm{d}\mathbf{v} \\
&= \left(\frac{g}{||\mathbf{v}||} \nabla_{\mathbf{w}}^T L - \frac{g\nabla_g L}{||\mathbf{v}||^2} \mathbf{v}^T\right)\mathrm{d}\mathbf{v}
\end{aligned}
$$
于是
$$
\nabla_{\mathbf{v}} L = \frac{g}{||\mathbf{v}||} \nabla_{\mathbf{w}} L - \frac{g\nabla_g L}{||\mathbf{v}||^2} \mathbf{v}
$$

**References**:
- [模型优化之Weight Normalization](https://zhuanlan.zhihu.com/p/55102378)
- [2016 NeurIPS] Weight normalization: A simple reparameterization to accelerate training of deep neural networks


# Data Augmentation

## AutoAugment
---
- [2018] AutoAugment: Learning Augmentation Policies from Data

## mixup
---
- [2017] mixup_ Beyond Empirical Risk Minimization
- [2018] Improved Mixed-Example Data Augmentation

## Manifold mixup
---
Manifold mixup 简言之就是特征映射上的 mixup.

- [2019 ICML] Manifold mixup: Better representations by interpolating hidden states

## SamplePairing
---
- [2018] Data Augmentation by Pairing Samples for Images Classification

## Unsupervised Data Augmentation
---
- [2019] Unsupervised Data Augmentation

## CutMix
---
CutMix 一般比 mixup 和 cutout 有效. 当数据集比较大时, cutmix 的效用不明显.

- [2019 ICCV]  CutMix: Regularization strategy to train strong classifiers with localizable features

## Mosaic
---
- [2020] YOLOv4_ Optimal Speed and Accuracy of Object Detection

## Scale Jittering
---
>> [2015 ICLR] Very deep convolutional networks for large-scale image recognition
> 
> The second approach to setting is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [S min, S max ] (we used > S min = 256 and S max = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training > set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales.

>> [2015] Deep Residual Learning for Image Recognition
> 
> The image is resized with its shorter side randomly sampled in [256,480] for scale augmentation. A 224×224 crop is randomly sampled from an image or its horizontal flip.

>> [2014 arXiv] Going deeper with convolutions
> 
> Still, one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly > between 8% and 100% of the image area and whose aspect ratio is chosen randomly between 3/4 and 4/3.

## [2020] Fixing the train-test resolution discrepancy
---
论文通过分析得出: 在图像分类任务中, 训练时和测试时数据增强不一致 (如训练时用的是 `RandomResizedCrop`, 而测试时用的是 `CenterCrop`) 会导致训练和测试时图像中目标的尺寸分布不一致, 这种不一致会导致性能下降.

## TTA, Test time augmentation
----
- use ten crops (one central, and one for each corner of the image and their mirrored versions).
    - [2016 CVPR] Deep residual learning for image recognition
    - [2012 NIPS] Imagenet classification with deep convolutional neural networks
    - [2015 CVPR] Going deeper with convolutions
- feeding image at multiple resolutions, again averaging the predictions.
    - [2016 CVPR] Deep residual learning for image recognition
    - [2015 CVPR] Going deeper with convolutions
    - [2015 ICLR] Very deep convolutional networks for large-scale image recognition


# Data Augmentation using Information Dropping
---

## Random Erasing
---
- [2017] Random erasing data augmentation

## cutout
---
- [2017] Improved regularization of convolutional neural networks with cutout

## Hide-and-seek (HaS)
---
divide the picture evenly into small squares and delete them randomly.

- [2018] Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond

## GridMask
---
- [202001] GridMask Data Augmentation
- https://github.com/akuxcw/GridMask


# Others
---
- [2017] L2 Regularization versus Batch and Weight Normalization
- [2017] Regularizing neural networks by penalizing confident output distributions
- [2017] Dataset Augmentation in Feature Space
- [2018] Data augmentation instead of explicit regularization
- [2018] Do deep nets really need weight decay and dropout
- https://github.com/takmin/DataAugmentation
- https://github.com/codebox/image_augmentor
- https://github.com/albumentations-team/albumentations

