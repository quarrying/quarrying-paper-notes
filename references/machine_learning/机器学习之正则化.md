# Regularization
## Weight Decay, L2 Regularization
----
- [1991 NIPS] A simple weight decay can improve generalization

## Dropout
----
- [2013 NIPS] Dropout Training as Adaptive Regularization
- [2014 JMLR] Dropout_ A Simple Way to Prevent Neural Networks from Overfitting

## DropConnect
----
- [2013 ICML] Regularization of Neural Networks using DropConnect

## LSR, Label Smoothing Regularization
---
- [2016 CVPR] Rethinking the Inception Architecture for Computer Vision
- [2017 ICLR] Regularizing Neural Networks By Penalizing Confident Output Distributions
- [2019] When Does Label Smoothing Help
- [202011] Delving Deep into Label Smoothing

## DisturbLabel
----
- [2016] DisturbLabel_ Regularizing CNN on the Loss Layer



## Noise
----
- [2017 NIPS] Regularizing Deep Neural Networks by Noise_ Its Interpretation and Optimization

## Feature Incay
---
- [2017] Feature Incay for Representation Regularization

## PatchShuffle
---
- [2017] PatchShuffle Regularization

## Shake-Shake
---
- [2017] Shake-Shake regularization

## SVB, Singular Value Bounding
----
- [2017 CVPR] Improving Training of Deep Neural Networks via Singular Value Bounding

## Shakeout
---
- [2017 TPAMI] Shakeout: A New Approach to Regularized Deep Neural Network Training

## VISER
---
- [2018] VISER_ Visual Self-Regularization

## ShakeDrop
---
- [2018] To realize a similar regularization to Shake-Shake on 1-branch network architectures

## Orthogonality Regularizations
---
- [2018] Can We Gain More from Orthogonality Regularizations in Training Deep CNNs

## DropBlock
---
- [2018 NIPS] DropBlock_ A regularization method for convolutional networks

## DropFilter
---
- [2018] DropFilter_ Dropout for Convolutions


# Normalization

## Batch Normalization
---
> BN uses mini-batch statistics during training and replace them with popular statistics during inference, introducing discrepancy between training and inference. Batch renormalization [21] was recently proposed to address this issue by gradually using popular statistics during training. As another interesting application of BN, Li et al. [34] found that BN can alleviate domain shifts by recomputing popular statistics in the target domain.
>> [2017 ICCV] Arbitrary style transfer in real-time with adaptive instance normalization

- [2015] Batch Normalization_ Accelerating Deep Network Training b y Reducing Internal Covariate Shift
- [2018] Batch Normalization and the impact of batch structure on the behavior of deep convolution networks
- [2018] Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift
- [2019] An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks
- [2018 NeurIPS] How Does Batch Normalization Help Optimization?

## Layer Normalization
----
- [2016] Layer Normalization

## Group Normalization
---
- [2018] Group Normalization


## Switchable Normalization
---
- [2018] Differentiable Learning-to-Normalize via Switchable Normalization


# Data Augmentation
- AutoAugment
    - [2018] AutoAugment: Learning Augmentation Policies from Data
- mixup
    - [2017] mixup_ Beyond Empirical Risk Minimization
    - [2018] Improved Mixed-Example Data Augmentation
- SamplePairing
    - [2018] Data Augmentation by Pairing Samples for Images Classification
- Unsupervised Data Augmentation
    - [2019] Unsupervised Data Augmentation
- CutMix
    - [2019 ICCV]  CutMix: Regularization strategy to train strong classifiers with localizable features
- Mosaic
    - [2020] YOLOv4_ Optimal Speed and Accuracy of Object Detection

## Information Dropping 数据增强
- Random Erasing
    - [2017] Random erasing data augmentation
- cutout
    - [2017] Improved regularization of convolutional neural networks with cutout
- Hide-and-seek (HaS): divide the picture evenly into small squares and delete them randomly.
    - [2018] Hide-and-seek: A data augmentation technique for weakly-supervised localization and beyond
- GridMask
    - [202001] GridMask Data Augmentation
    - https://github.com/akuxcw/GridMask


# Others
- [2017] L2 Regularization versus Batch and Weight Normalization
- [2017] Regularizing neural networks by penalizing confident output distributions
- [2017] Dataset Augmentation in Feature Space
- [2018] Data augmentation instead of explicit regularization
- [2018] Do deep nets really need weight decay and dropout
- https://github.com/takmin/DataAugmentation
- https://github.com/codebox/image_augmentor
- https://github.com/albumentations-team/albumentations


    
