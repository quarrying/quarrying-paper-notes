# Distillation

## Awesome Knowledge Distillation
---
- https://github.com/dkozlov/awesome-knowledge-distillation

## [2014 NIPS] Do Deep Nets Really Need to be Deep
---

## [2014 NIPS] Distilling the Knowledge in a Neural Network
---

## [2015 ICLR] FitNets_ Hints for Thin Deep Nets
---

## [2016] Deep Model Compression_ Distilling Knowledge from Noisy Teachers
---

## [2016 AAAI] Face Model Compression by Distilling Knowledge from Neurons
---

## [2017 CVPR] A Gift from Knowledge Distillation_ Fast Optimization, Network Minimization and Transfer Learning
---

## [2017 ICLR] Paying More Attention to Attention_ Improving the Performance of Convolutional Neural Networks via Attention Transfer
---

## [2017] Efficient Knowledge Distillation from an Ensemble of Teachers
---

## [2017] Like What You Like_ Knowledge Distill via Neuron Selectivity Transfer
---

## [2018 AAAI] Rocket Launching_ A Universal and Efficient Framework for Training Well-performing Light Net
---

## [2018] Feature Matters_ A Stage-by-Stage Approach for Knowledge Transfer
---

## [2018 ECCV] Self-supervised knowledge distillation using singular value decomposition
---

## [2019 AAAI] Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons
---

## [2019] Feature Fusion for Online Mutual Knowledge Distillation
---

## [2019] Knowledge Distillation via Route Constrained Optimization
---

## [2019] Relational Knowledge Distillation
---

## [2019] Triplet Distillation for Deep Face Recognition
---

## [2020] Knowledge Distillation Meets Self-Supervision
---

## [2020] Channel Distillation_ Channel-Wise Attention for Knowledge Distillation
----

# Quantization

## Overview
---
- http://chenrudan.github.io/blog/2018/10/02/networkquantization.html
- [2017] A Survey of Model Compression and Acceleration for Deep Neural Networks

## Deep Compression
---
- [2016 ICLR] Deep Compression_ Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding

## [2015 NIPS] BinaryConnect: Training Deep Neural Networks with binary weights during propagations
---
- https://github.com/MatthieuCourbariaux/BinaryConnect

## [2016] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1
---

## [2016] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Network
---
- http://allenai.org/plato/xnornet

## [2017] Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
---

## Song Han et al.
---
- [2015 NIPS] Learning both Weights and Connections for Efficient Neural Networks

## EIE
---
- [2016] EIE_ Efficient Inference Engine on Compressed Deep Neural Network

## Integer-Arithmetic-Only
---
- [2017] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference

## caffe-int8-convert-tools
---
- https://github.com/BUG1989/caffe-int8-convert-tools

## Ristretto
---
- [Ristretto Overview](http://lepsucd.com/?page_id=621)
- [Ristretto Github Repo](https://github.com/pmgysel/caffe)

## [2015 ICML] Compressing neural networks with the hashing trick
---

## [1993] Svd-net: an algorithm that automatically selects network structure
---

## [2015] Fast algorithms for convolutional neural networks
---

## [2014] Minimizing Computation in Convolutional Neural Networks
---

# Pruning

## [[2018] Progressive Deep Neural Networks Acceleration via Soft Filter Pruning](https://arxiv.org/pdf/1808.07471.pdf)
---

## [[2018] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks](https://arxiv.org/pdf/1808.06866.pdf)
---

## [[2017] To prune, or not to prune: exploring the efficacy of pruning for model compression](https://arxiv.org/pdf/1710.01878.pdf)
---

## [2018] Rethinking the Value of Network Pruning
---

## [2018] Pruning neural networks: is it time to nip it in the bud?
---

## [2018] Dynamic Channel Pruning_ Feature Boosting and Suppression
---

## Awesome Pruning
---
- https://github.com/he-y/Awesome-Pruning

