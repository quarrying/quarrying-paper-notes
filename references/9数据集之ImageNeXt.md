注: ImageNeXt 是自拟的单词, 它表示: 和 ImageNet 一样, 属于图像分类数据集, 同时一般具有比 ImageNet 更多的类别和图像.

# JFT-300M
JFT-300M 是谷歌内部数据集.

JFT-300M consists of around 300 million noisily labelled images with 1.26 labels per image on average. The labels are organized into a hierarchy of 18291 classes. Annotation is performed using an automatic pipeline, and are therefore imperfect; approximately 20% of the labels are noisy.

**References**:
- [2017 ICCV] Revisiting unreasonable effectiveness of data in deep learning era


# Instagram-1B
**References**
- [2019] Billion-scale semi-supervised learning for image classification

#  YFCC100M, YFCC
----
- [2016] Yfcc100m: The new data in multimedia research

# Open Images

## Open Images Dataset V5, OIDV5
> Open Images is a dataset of ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, and visual relationships. It contains a total of 16M bounding boxes for 600 object classes on 1.9M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.3 per image on average). Open Images also offers visual relationship annotations, indicating pairs of objects in particular relations (e.g. "woman playing guitar", "beer on table"). In total it has 329 relationship triplets with 391,073 samples. In V5 we added segmentation masks for 2.8M object instances in 350 classes. Segmentation masks mark the outline of objects, which characterizes their spatial extent to a much higher level of detail. Finally, the dataset is annotated with 36.5M image-level labels spanning 19,969 classes.

## Open Images Dataset V4, OIDV4
> Open Images is a dataset of ~9M images that have been annotated with image-level labels, object bounding boxes and visual relationships. The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). This also encorages structural image annotations, such as visual relationships. Moreover, the dataset is annotated with image-level labels spanning thousands of classes.

**References**:
- https://github.com/openimages/dataset
- http://storage.googleapis.com/openimages/web/index.html
- http://storage.googleapis.com/openimages/web/challenge.html

# WebVision
**References**:
- https://data.vision.ee.ethz.ch/cvl/webvision/index.html

