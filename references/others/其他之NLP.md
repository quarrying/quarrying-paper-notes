# Papers

## [2019] BERT
---
non-causal language model

- [2019]] Bert: Pre-training of deep bidirectional transformers for language understanding

## [2019] GTP-2
----
WebText 的训练集为 WebText.

- [2019] Language models are unsupervised multitask learners

## [2020] Scaling laws for neural language models
----

## [2020] GPT-3
----
causal language model

- [2020 NeurIPS] Language models are few-shot learners

## [2018] ELMo
----
- [2018 NAACL] Deep contextualized word representations

## [2019] XLNet
----
- [2019 NeurIPS] Xlnet: Generalized autoregressive pretraining for language understanding

## [2019] RoBERTa
---
- [2019] Roberta: A robustly optimized bert pretraining approach

## [2020] ALBERT
---
- [2020 ICLR] Albert: Alite bert for self-supervised learning of language representations


# Datasets

## WMT 2014 English-to-German
---

## WMT 2014 English-to-French
----

