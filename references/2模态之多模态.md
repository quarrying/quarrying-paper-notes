# Tasks
- Visual Grounding (VG): 视觉定位.
- Grounded Captioning (GC)
- Image Captioning (IC): automatically generating a natural language description for a given image.
- Visual Question Answering (VQA): 视觉问答.
- Visual Dialogue: 视觉问答.
- Visual Reasoning (VR): 视觉推理. 
- Visual Commonsense Reasoning (VCR): 视觉常识推理. 
- Visual Entailment (VE): 视觉蕴涵.
- Image-Text Retrieval (ITR)
- Image-Text Matching (ITM)
- Referring Expression Comprehension
- Word-Region Alignment (WRA)
- Natural Language for Visual Reasoning (NLVR^2): asks the model to predict whether a sentence describes a pair of images.
- phrase grounding (object referring): 给出一张图片和一个自然语言描述的问题, 在图片中定位问题中所提到的物体.
- Temporal Activity Localization by Language: 给定一个query (包含对 activity 的描述), 找到对应动作 (事件) 的起止时间.
- Spatio-temporal object referring by language: 给定一个query (包含对 object/person 的描述), 在时空中找到连续的 bounding box (也就是一个 tube).
- Temporal video grounding (TVG): aims to localize a target segment in a video according to a given sentence query. 
- Vision and language pre-training (VLP)
- Text-to-image generation, Text-to-image synthesis, Text-guided Image Editing

Notes:
- Temporal Activity Localization by Language 实际上就是下文中的 TVG?


# VLP, Vision and Language Pre-training

## [2019] VisualBERT
---
- [2019] VisualBERT: a simple and performant baseline for vision and language

## [2021] CLIP, Contrastive Language-Image Pre-training
----
CLIP 自行构建了一个新的数据集 WebImageText, 简称为 WIT (WIT-400M).

CLIP 使用 image-text matching 作为预训练任务. 损失函数称为 image-text contrastive loss, 简称为 ITC.

CLIP uses an autoregressive text encoder (GPT-2?), a BytePairEncoding tokenizer, and a length of 77.

CLIP image encoder use ViT-L/14@336px which found to perform best.

generative objective 比 contrastive objective 更耗时.

- [2021] Learning Transferable Visual Models From Natural Language Supervision
- https://github.com/openai/CLIP
- https://github.com/ofa-sys/chinese-clip


## [2021 ICML] ALIGN, A Large-scale ImaGe and Noisy-text embedding
----
> Besides using different vision and language encoder
> architectures, the key difference is on training data: ALIGN
> follows the natural distribution of image-text pairs from the
> raw alt-text data, while CLIP collects the dataset by first
> constructing an allowlist of high-frequency visual concepts
> from English Wikipedia.

ALIGN 和 CLIP 并无太大差异, 主要差异在于训练集数据构造方式.

- [2021 ICML] Scaling up visual and vision-language representation learning with noisy text supervision

## [2022 ICML] OFA
---
- [2022 ICML] OFA_ Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework

## [2021] ALBEF
---
- [2021] Align before Fuse_ Vision and Language Representation Learning with Momentum Distillation

## [2021] VLMO
----
- [2021] VLMo_ Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

## [2022] LiT, Locked-image text Tuning
----
简言之: 图像编码器用预训练模型初始化且冻结, 文本编码器随机初始化且可训练, 任务为 Contrastive pre-training (和 CLIP 一样).

> This indicates that locking the image tower during tuning, i.e. LiT, leads to a text model that is well aligned to an already strong and general image representation, as opposed to an image-text model that is well aligned but specialized to the dataset used for alignment.

- [2022] LiT_ Zero-Shot Transfer with Locked-image text Tuning

## [2022] BLIP, [2023] BLIP-2
----
BLIP 提出一个名为 CapFilt (Captioning and Filtering) 的方法, 从含噪的图文对中构建数据.

- [2022] BLIP_ Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- [2023] BLIP-2_ Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

## [2022] OpenCLIP
---
基于开源的 LAION 数据集和 OpenCLIP 代码库来研究 CLIP 的 scaling laws, 并开源了训练代码及结果模型.

- https://github.com/mlfoundations/open_clip
- [2022] Reproducible scaling laws for contrastive language-image learning

## [2022 CVPR] UniCL
---
简言之: 建立 image, text 和 label 的语义对齐?

- [2022 CVPR] Unified Contrastive Learning in Image-Text-Label Space
- https://github.com/microsoft/UniCL

## [2021] Florence
----
- [2021] Florence: A New Foundation Model for Computer Vision

## [2022] XCLIP, X-CLIP, X-Florence
----
在 CLIP 模型上添加若干模块, 用于视频分类.

- [2022] Expanding Language-Image Pretrained Models for General Video Recognition

## [2022] FLIP, Fast Language-Image Pre-training
---
- [2022] Scaling Language-Image Pre-training via Masking

## [2022] VLKD, vision-language knowledge distillation
---
- [2022] Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation

## [2022] Flamingo
----
基本思想: 图像预训练模型可训练, 文本预训练模型冻结, 任务为 vision-to-language generation?

- [2022] Flamingo_ a Visual Language Model for Few-Shot Learning

## miniGPT-4
---
- [2023] Minigpt-4: Enhancing vision-language understanding with advanced large language models

## LLaVa, Language-and-Vision Assistant
---
- [2023 NeurIPS] Visual instruction tuning
- https://llava-vl.github.io/

## [2022 CVPR] CLIPSeg
----
- [2022 CVPR] Image Segmentation Using Text and Image Prompts
- https://github.com/timojl/clipseg

## [2021] ViLD
----
利用 CLIP 来实现零样本目标检测, 具体地: 使用 CLIP 的 text encoder 提取标签的特征作为 classifier weight, 同时利用 CLIP 的 image encoder 提取proposal 特征来蒸馏 detector rpn 提取的 proposal 特征.

- [2021] Zero-shot detection via vision and language knowledge distillation
- https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild

## [2021] RegionCLIP
----
文章通过实验发现: CLIP的image encoder提取的特征的定域性很差(紧贴的框比扩边的框的效果还差), 不适用于目标检测任务适配. 

本文利用 CLIP 构建了 region-text pair 数据集, 并使用 region-text matching 作为预训练任务.

该文没有对语言模型进行训练:
> Our method relies on CLIP’s visual-semantic space and has not updated the language encoder. When given similar scale of data as CLIP, unfreezing the language encoder may bring more gain in our region-based language-image pretraining.

- [2021] RegionCLIP_ Region-based Language-Image Pretraining
- https://github.com/microsoft/RegionCLIP

## DenseCLIP
---
将 CLIP 适配于稠密预测任务. 具体思路: 将 CLIP 中的 image-text matching 任务转化为 pixel-text matching 任务, 并通过 context-aware prompting 来提高文本特征的表示能力.

- [2022] DenseCLIP_ Language-Guided Dense Prediction with Context-Aware Prompting
- https://github.com/raoyongming/DenseCLIP

## segment-anything-with-clip
---
将 CLIP 和 SAM 适配于语义分割. 具体思路: 利用 SAM 提取目标区域, 利用 CLIP 的 image encoder 提取目标区域的特征, 并利用 CLIP 的 text encoder 提取标签特征.

- https://github.com/Curt-Park/segment-anything-with-clip
- https://huggingface.co/spaces/curt-park/segment-anything-with-clip

## GLIP, Grounded Language-Image Pre-training
---
GLIP 利用 phrase grounding 作为预训练任务, 是比 image-text matching 任务更强的监督信号. 该文献统一了 phrase grounding 和 object detection (实际上是将 object detection 任务转化为 phrase grounding 任务).

- [2021] Grounded Language-Image Pre-training
- https://github.com/microsoft/GLIP

## MaskCLIP
---
提出的预训练任务集合了 image-text matching, masked self-distillation 和 local semantic learning.

- [2023] MaskCLIP_ Masked Self-Distillation Advances Contrastive Language-Image Pretraining
- https://github.com/LightDXY/MaskCLIP 

## mPLUG
---

# Text-to-image generation

## [2022] DALL·E, DALLE, DALL·E 2
----
DALL·E 中使用了 CLIP.

- [2022] Hierarchical text-conditional image generation with clip latents

## [2022 NIPS] Imagegen
---
- [2022 NIPS] Photorealistic text-to-image diffusion models with deep language understanding

## [2022] Parti
---
- [2022] Scaling autoregressive models for content-rich text-to-image generation

## [2022] Textual-Inversion
----
- [2022] An image is worth one word: Personalizing text-to-image generation using textual inversion

## [2022] DreamBooth
----
- [2022] Dreambooth_ Finetuning text-to-image diffusion models for subject-driven generation
- <https://huggingface.co/spaces/carlgira/dreambooth-image-editor>

## [2022] SINE, SINgle Image Editing
----
- [2022] SINE_ SINgle Image Editing with Text-to-Image Diffusion Models

## [2021] Cogview
---
- [2021] Cogview: Mastering text-to-image generation via transformers

## [2021] Zero-shot text-to-image generation
----

## [2021] LAFITE
----
- [2021] LAFITE: towards language-free training for text-to-image generation

## [2021] Make-a-scene: Scene-based text-to-image generation with human priors
-----

## [2021] GLIDE
----
- [2021] GLIDE: towards photorealistic image generation and editing with text-guided diffusion models

## [2023] ImageBind
---
- [2023] ImageBind: One Embedding Space To Bind Them All


# Datasets & Challenges

## Person In Context
----
- http://picdataset.com/challenge/index/

## ActivityNet Captions
----

## Charades-STA
----
- [2016 ECCV] Hollywood in homes: Crowdsourcing data collection for activity understanding

## Visual Genome, VG
----
- [2017 IJCV] Visual genome_ Connecting language and vision using crowdsourced dense image annotations
- https://visualgenome.org/

## Conceptual Captions, CC, CC3M
----
该文献提出了一个 image-text 数据集的构造方法, 这个方法在后续的文献中并广泛借鉴 (例如 ALIGN).

- [2018 ACL] Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning
- https://github.com/google-research-datasets/conceptual-captions

## SBU Captions
----
- [2011 NeurIPS] Im2text: Describing images using 1 million captioned photographs
- http://dsl1.cewit.stonybrook.edu/~vicente/sbucaptions/

## COCO captions
----
> containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

1 张图像对应 5 段文本.

- [2015] Microsoft COCO captions: Data collection and evaluation server

## TextCaps
----
- [2020 ECCV] TextCaps: a Dataset for Image Captioningwith Reading Comprehension

## nocaps, novel object captioning at scale
----
- https://nocaps.org/

## LAION, LAION-400M, LAION-5B, LAION-2B
----
LAION-2B: an English image-text subset of 2.32 billion samples of LAION-5B.

- [2021] Laion-400m_ Open dataset of clip-filtered 400 million image-text pairs
- [2022] LAION-5B_ An open large-scale dataset for training next generation image-text models
- https://laion.ai/blog/laion-5b/


## Wukong, Noah-Wukong
-----
华为诺亚方舟开源的中文多模态数据集. 

The dataset contains 100 Million image-text pairs

- https://wukong-dataset.github.io/wukong-dataset/

## Flickr8k, Flickr8k-CN, Flickr 8k
-----
- [2013] Framing image description as a ranking task: Data, models and evaluation metrics
- [2016] Adding Chinese Captions to Images

## Flickr30k, Flickr 30k
-----
图文检索数据集, 目前指标已经刷的很高了.

> an image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. This is an extension of our previous Flickr 8k Dataset. The new images and captions focus on people involved in everyday activities and events.

- [2014] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
- https://shannon.cs.illinois.edu/DenotationGraph/

## Conceptual 12M, CC12M
---
> CC12M is on purpose designed for vision-and-language pre-training, and meant to be disjoint from CC3M. CC3M is cleaner and more appropriate for fine-tuning, but can be used in conjunction with CC12M for pre-training, as illustrated in our paper. Coincidentally, their intersection is found to be non-zero — approximately 63K URLs.

- [2021 CVPR] Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts
- https://github.com/google-research-datasets/conceptual-12m


## COYO-700M, COYO-Labeled-300M
----
> COYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models.
> We also provide COYO-Labeled-300M by adding machine-generated vision labels to a subset of COYO-700M for comparison with the JFT-300M.

- https://github.com/kakaobrain/coyo-dataset
- [2022] COYO-700M: Image-Text Pair Dataset 


## WIT, Wikipedia-based Image Text Dataset
----
> Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages.

注意 CLIP 的训练集也成为 WIT (WebImageText).

- https://github.com/google-research-datasets/wit
- [2021] WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning


## YFCC100M-CLIP
---
> YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automatically generated filenames like 20160716_113957.JPG as “titles” or contain “descriptions” of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos.

- [2021] Learning transferable visual models from natural language supervision

## Common Crawl, CommonCrawl
----
- https://commoncrawl.org

## MUGE, Multimodal Understanding and Generation Evaluation Benchmark
---
- https://tianchi.aliyun.com/muge

## WaveCaps
----
> which contains 403,050 audio clips with average duration of 67.59 seconds and average caption length of 7.8 words. It combines four datasets including FreeSound (262,300) [31], BBC Sound Effects (31,201), SoundBible (1,231) and AudioSet strongly-labelled subset (108,317), and transform their raw-descriptions into captions with ChatGPT.

- [2023] Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research

## Zero
----
> Zero, a large-scale Chinese cross-modal benchmark, contains two pre-training datasets called Zero-Corpus and five downstream datasets.

- https://zero.so.com/index.html

