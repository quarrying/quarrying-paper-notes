# Tasks
- Visual Grounding (VG)
- Grounded Captioning (GC)
- Image-Text Matching (ITM)
- Image Captioning (IC)
    - automatically generating a natural language description for a given image
- Visual Question Answering (VQA)
- Image-Text Retrieval
- Referring Expression Comprehension
- Word-Region Alignment (WRA)
- Visual Commonsense Reasoning (VCR)
- Visual Entailment
- Natural Language for Visual Reasoning (NLVR^2)
- phrase grounding (object referring): 
    - 给出一张图片和一个自然语言描述的问题, 在图片中定位问题中所提到的物体.
- Temporal Activity Localization by Language: 
    - 给定一个query (包含对 activity 的描述), 找到对应动作 (事件) 的起止时间.
    - Temporal Activity Localization by Language 实际上就是下文中的 TVG?
- Spatio-temporal object referring by language: 
    - 给定一个query (包含对 object/person 的描述), 在时空中找到连续的 bounding box (也就是一个 tube).
- Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. 
- vision and language pre-training (VLP)
- Text-to-image generation, Text-to-image synthesis, Text-guided Image Editing


# Papers

## [2022] Language Models are General-Purpose Interfaces
---

## [2022 ICML] OFA
---
- [2022 ICML] OFA_ Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework

## [2021] CLIP, Contrastive Language-Image Pre-training
----
自行构建了一个新的数据集 WebImageText, 简称为 WIT.

> CLIP is pre-trained to predict if an image and a text snippet are paired together in its dataset.

- [2021] Learning Transferable Visual Models From Natural Language Supervision

## [2021 ICML] ALIGN, A Large-scale ImaGe and Noisy-text embedding
----
> Besides using different vision and language encoder
> architectures, the key difference is on training data: ALIGN
> follows the natural distribution of image-text pairs from the
> raw alt-text data, while CLIP collects the dataset by first
> constructing an allowlist of high-frequency visual concepts
> from English Wikipedia.

ALIGN 和 CLIP 并无太大差异, 主要差异在于训练集数据构造方式.

- [2021 ICML] Scaling up visual and vision-language representation learning with noisy text supervision


## [2022] DALL·E,  DALLE, DALL·E 2
----
- [2022] Hierarchical text-conditional image generation with clip latents

## [2022 NIPS] Imagegen
---
- [2022 NIPS] Photorealistic text-to-image diffusion models with deep language understanding

## [2022] Parti
---
- [2022] Scaling autoregressive models for content-rich text-to-image generation

## [2022] Textual-Inversion
----
- [2022] An image is worth one word: Personalizing text-to-image generation using textual inversion

## [2022] DreamBooth
----
- [2022] Dreambooth_ Finetuning text-to-image diffusion models for subject-driven generation
- <https://huggingface.co/spaces/carlgira/dreambooth-image-editor>

## [2022] SINE, SINgle Image Editing
----
- [2022] SINE_ SINgle Image Editing with Text-to-Image Diffusion Models

## [2021] Cogview
---
- [2021] Cogview: Mastering text-to-image generation via transformers

## [2021] Zero-shot text-to-image generation
----
- [2021] Zero-shot text-to-image generation

## [2021] LAFITE
----
- [2021] LAFITE: towards language-free training for text-to-image generation

## [2021] Make-a-scene: Scene-based text-to-image generation with human priors
-----
- [2021] Make-a-scene: Scene-based text-to-image generation with human priors

## [2021] GLIDE
----
- [2021] GLIDE: towards photorealistic image generation and editing with text-guided diffusion models

## [2022] LiT, Locked-image text Tuning
----
简言之: 图像编码器用预训练模型初始化且冻结, 文本编码器随机初始化且可训练.

> This indicates that locking the image tower during tuning, i.e. LiT, leads to a text model that is well aligned to an already strong and general image representation, as opposed to an image-text model that is well aligned but specialized to the dataset used for alignment.

- [2022] LiT_ Zero-Shot Transfer with Locked-image text Tuning

## [2022] BLIP, [2023] BLIP-2
----
- [2022] BLIP_ Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- [2023] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models


# Datasets & Challenges

## Person In Context
----
- http://picdataset.com/challenge/index/

## ActivityNet Captions
----

## Charades-STA
----
- [2016 ECCV] Hollywood in homes: Crowdsourcing data collection for activity understanding

## Visual Genome, VG
----
- [2017 IJCV] Visual genome_ Connecting language and vision using crowdsourced dense image annotations
- https://visualgenome.org/

## Conceptual Captions, CC, CC3M
----
该文献提出了一个 image-text 数据集的构造方法, 这个方法在后续的文献中并广泛借鉴 (例如 ALIGN).

- [2018 ACL] Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning
- https://github.com/google-research-datasets/conceptual-captions

## SBU Captions
----
- [2011 NeurIPS] Im2text: Describing images using 1 million captioned photographs
- http://dsl1.cewit.stonybrook.edu/~vicente/sbucaptions/

## COCO captions
----
> containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

- [2015] Microsoft COCO captions: Data collection and evaluation server

## TextCaps
----
- [2020 ECCV] TextCaps: a Dataset for Image Captioningwith Reading Comprehension

## nocaps, novel object captioning at scale
----
- https://nocaps.org/

## LAION, LAION-400M, LAION-5B
----
- [2021] Laion-400m: Open dataset of clip-filtered 400 million image-text pairs

## Wukong, Noah-Wukong
-----
华为诺亚方舟开源的中文多模态数据集. 

The dataset contains 100 Million <image, text> pairs

- https://wukong-dataset.github.io/wukong-dataset/

## Flickr8k, Flickr8k-CN
-----
- [2013] Framing image description as a ranking task: Data, models and evaluation metrics
- [2016] Adding Chinese Captions to Images

## Flickr30k
-----
- [2014] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions

## Conceptual 12M, CC12M
---
> CC12M is on purpose designed for vision-and-language pre-training, and meant to be disjoint from CC3M. CC3M is cleaner and more appropriate for fine-tuning, but can be used in conjunction with CC12M for pre-training, as illustrated in our paper. Coincidentally, their intersection is found to be non-zero — approximately 63K URLs.

- [2021 CVPR] Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts
- https://github.com/google-research-datasets/conceptual-12m

## COYO-700M
----
- https://github.com/kakaobrain/coyo-dataset

## WIT, Wikipedia-based Image Text Dataset
----
> Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages.

- https://github.com/google-research-datasets/wit
- [2021] WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning


