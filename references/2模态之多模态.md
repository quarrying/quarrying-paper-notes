# Tasks
- Visual Grounding (VG): 视觉定位.
- Grounded Captioning (GC)
- Image Captioning (IC): automatically generating a natural language description for a given image.
- Visual Question Answering (VQA): 视觉问答.
- Visual Reasoning (VR): 视觉推理. 
- Visual Commonsense Reasoning (VCR): 视觉常识推理. 
- Visual Entailment (VE): 视觉蕴涵.
- Image-Text Retrieval (ITR)
- Image-Text Matching (ITM)
- Referring Expression Comprehension
- Word-Region Alignment (WRA)
- Natural Language for Visual Reasoning (NLVR^2)
- phrase grounding (object referring): 给出一张图片和一个自然语言描述的问题, 在图片中定位问题中所提到的物体.
- Temporal Activity Localization by Language: 给定一个query (包含对 activity 的描述), 找到对应动作 (事件) 的起止时间.
- Spatio-temporal object referring by language: 给定一个query (包含对 object/person 的描述), 在时空中找到连续的 bounding box (也就是一个 tube).
- Temporal video grounding (TVG): aims to localize a target segment in a video according to a given sentence query. 
- Vision and language pre-training (VLP)
- Text-to-image generation, Text-to-image synthesis, Text-guided Image Editing

Notes:
- Temporal Activity Localization by Language 实际上就是下文中的 TVG?


# VLP, Vision and Language Pre-training

## [2019] VisualBERT
---
- [2019] VisualBERT: a simple and performant baseline for vision and language

## [2021] CLIP, Contrastive Language-Image Pre-training
----
CLIP 自行构建了一个新的数据集 WebImageText, 简称为 WIT (WIT-400M).

CLIP 的损失函数称为 image-text contrastive loss, 简称为 ITC.

CLIP uses an autoregressive text encoder (GPT-2?), a BytePairEncoding tokenizer, and a length of 77.

CLIP image encoder use ViT-L/14@336px which found to perform best.

generative objective 比 contrastive objective 更耗时.

- [2021] Learning Transferable Visual Models From Natural Language Supervision
- https://github.com/openai/CLIP
- https://github.com/ofa-sys/chinese-clip


## [2021 ICML] ALIGN, A Large-scale ImaGe and Noisy-text embedding
----
> Besides using different vision and language encoder
> architectures, the key difference is on training data: ALIGN
> follows the natural distribution of image-text pairs from the
> raw alt-text data, while CLIP collects the dataset by first
> constructing an allowlist of high-frequency visual concepts
> from English Wikipedia.

ALIGN 和 CLIP 并无太大差异, 主要差异在于训练集数据构造方式.

- [2021 ICML] Scaling up visual and vision-language representation learning with noisy text supervision

## [2022 ICML] OFA
---
- [2022 ICML] OFA_ Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework

## [2021] ALBEF
---
- [2021] Align before Fuse_ Vision and Language Representation Learning with Momentum Distillation

## [2021] VLMO
----
- [2021] VLMo_ Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

## [2022] LiT, Locked-image text Tuning
----
简言之: 图像编码器用预训练模型初始化且冻结, 文本编码器随机初始化且可训练, 任务为 Contrastive pre-training (和 CLIP 一样).

> This indicates that locking the image tower during tuning, i.e. LiT, leads to a text model that is well aligned to an already strong and general image representation, as opposed to an image-text model that is well aligned but specialized to the dataset used for alignment.

- [2022] LiT_ Zero-Shot Transfer with Locked-image text Tuning

## [2022] BLIP, [2023] BLIP-2
----
BLIP 提出一个名为 CapFilt (Captioning and Filtering) 的方法, 从含噪的图文对中构建数据.

- [2022] BLIP_ Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- [2023] BLIP-2_ Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

## [2022] OpenCLIP
---
基于开源的 LAION 数据集和 OpenCLIP 代码库来研究 CLIP 的 scaling laws, 并开源了训练代码及结果模型.

- https://github.com/mlfoundations/open_clip
- [2022] Reproducible scaling laws for contrastive language-image learning

## [2022 CVPR] UniCL
---
简言之: 建立 image, text 和 label 的语义对齐?

- [2022 CVPR] Unified Contrastive Learning in Image-Text-Label Space
- https://github.com/microsoft/UniCL

## [2021] Florence
----
- [2021] Florence: A New Foundation Model for Computer Vision

## [2022] XCLIP, X-CLIP, X-Florence
----
在 CLIP 模型上添加若干模块, 用于视频分类.

- [2022] Expanding Language-Image Pretrained Models for General Video Recognition

## [2022] FLIP, Fast Language-Image Pre-training
---
- [2022] Scaling Language-Image Pre-training via Masking

## [2022] VLKD, vision-language knowledge distillation
---
- [2022] Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation

## [2022] Flamingo
----
基本思想: 图像预训练模型可训练, 文本预训练模型冻结, 任务为 vision-to-language generation?

- [2022] Flamingo_ a Visual Language Model for Few-Shot Learning

## miniGPT-4
---

## LLaVa, Language-and-Vision Assistant
---


# CLIP derivatives

## [2022 CVPR] CLIPSeg
----
- [2022 CVPR] Image Segmentation Using Text and Image Prompts
- https://github.com/timojl/clipseg


# Text-to-image generation

## [2022] DALL·E, DALLE, DALL·E 2
----
DALL·E 中使用了 CLIP.

- [2022] Hierarchical text-conditional image generation with clip latents

## [2022 NIPS] Imagegen
---
- [2022 NIPS] Photorealistic text-to-image diffusion models with deep language understanding

## [2022] Parti
---
- [2022] Scaling autoregressive models for content-rich text-to-image generation

## [2022] Textual-Inversion
----
- [2022] An image is worth one word: Personalizing text-to-image generation using textual inversion

## [2022] DreamBooth
----
- [2022] Dreambooth_ Finetuning text-to-image diffusion models for subject-driven generation
- <https://huggingface.co/spaces/carlgira/dreambooth-image-editor>

## [2022] SINE, SINgle Image Editing
----
- [2022] SINE_ SINgle Image Editing with Text-to-Image Diffusion Models

## [2021] Cogview
---
- [2021] Cogview: Mastering text-to-image generation via transformers

## [2021] Zero-shot text-to-image generation
----

## [2021] LAFITE
----
- [2021] LAFITE: towards language-free training for text-to-image generation

## [2021] Make-a-scene: Scene-based text-to-image generation with human priors
-----

## [2021] GLIDE
----
- [2021] GLIDE: towards photorealistic image generation and editing with text-guided diffusion models

## [2023] ImageBind
---
- [2023] ImageBind: One Embedding Space To Bind Them All


# Datasets & Challenges

## Person In Context
----
- http://picdataset.com/challenge/index/

## ActivityNet Captions
----

## Charades-STA
----
- [2016 ECCV] Hollywood in homes: Crowdsourcing data collection for activity understanding

## Visual Genome, VG
----
- [2017 IJCV] Visual genome_ Connecting language and vision using crowdsourced dense image annotations
- https://visualgenome.org/

## Conceptual Captions, CC, CC3M
----
该文献提出了一个 image-text 数据集的构造方法, 这个方法在后续的文献中并广泛借鉴 (例如 ALIGN).

- [2018 ACL] Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning
- https://github.com/google-research-datasets/conceptual-captions

## SBU Captions
----
- [2011 NeurIPS] Im2text: Describing images using 1 million captioned photographs
- http://dsl1.cewit.stonybrook.edu/~vicente/sbucaptions/

## COCO captions
----
> containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).

1 张图像对应 5 段文本.

- [2015] Microsoft COCO captions: Data collection and evaluation server

## TextCaps
----
- [2020 ECCV] TextCaps: a Dataset for Image Captioningwith Reading Comprehension

## nocaps, novel object captioning at scale
----
- https://nocaps.org/

## LAION, LAION-400M, LAION-5B, LAION-2B
----
LAION-2B: an English image-text subset of 2.32 billion samples of LAION-5B.

- [2021] Laion-400m_ Open dataset of clip-filtered 400 million image-text pairs
- [2022] LAION-5B_ An open large-scale dataset for training next generation image-text models
- https://laion.ai/blog/laion-5b/


## Wukong, Noah-Wukong
-----
华为诺亚方舟开源的中文多模态数据集. 

The dataset contains 100 Million <image, text> pairs

- https://wukong-dataset.github.io/wukong-dataset/

## Flickr8k, Flickr8k-CN
-----
- [2013] Framing image description as a ranking task: Data, models and evaluation metrics
- [2016] Adding Chinese Captions to Images

## Flickr30k
-----
图文检索数据集, 目前指标已经刷的很高了.

- [2014] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions


## Conceptual 12M, CC12M
---
> CC12M is on purpose designed for vision-and-language pre-training, and meant to be disjoint from CC3M. CC3M is cleaner and more appropriate for fine-tuning, but can be used in conjunction with CC12M for pre-training, as illustrated in our paper. Coincidentally, their intersection is found to be non-zero — approximately 63K URLs.

- [2021 CVPR] Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts
- https://github.com/google-research-datasets/conceptual-12m


## COYO-700M, COYO-Labeled-300M
----
> COYO-700M is a large-scale dataset that contains 747M image-text pairs as well as many other meta-attributes to increase the usability to train various models.
> We also provide COYO-Labeled-300M by adding machine-generated vision labels to a subset of COYO-700M for comparison with the JFT-300M.

- https://github.com/kakaobrain/coyo-dataset
- [2022] COYO-700M: Image-Text Pair Dataset 


## WIT, Wikipedia-based Image Text Dataset
----
> Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages.

注意 CLIP 的训练集也成为 WIT (WebImageText).

- https://github.com/google-research-datasets/wit
- [2021] WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning


## YFCC100M-CLIP
---
> YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automatically generated filenames like 20160716_113957.JPG as “titles” or contain “descriptions” of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos.

- [2021] Learning transferable visual models from natural language supervision

## Common Crawl, CommonCrawl
----
- https://commoncrawl.org.

## MUGE, Multimodal Understanding and Generation Evaluation Benchmark
---
- https://tianchi.aliyun.com/muge

