# Tasks
- sentiment analysis
- topic detection
- language detection
- key phrase extraction
- document categorization
- question answering
- machine translation
- reading comprehension
- summarization

# Papers

## [2019] BERT
---
non-causal language model

- BERT base
- BERT large

- [2019]] Bert: Pre-training of deep bidirectional transformers for language understanding

## [2018] GPT, generative pre-training
----
- [2018] Improving language understanding by generative pre-training


## [2019] GTP-2
----
GTP-2 的训练集为 WebText.

Parameters | Layers | $d_{model}$
-----------|--------|--------------
117M       | 12     | 768
345M       | 24     | 1024
762M       | 36     | 1280
1542M      | 48     | 1600

- [2019] Language models are unsupervised multitask learners

## [2020] Scaling laws for neural language models
----

## [2020] GPT-3
----
causal language model

- [2020 NeurIPS] Language models are few-shot learners

## [2018] ELMo
----
- [2018 NAACL] Deep contextualized word representations

## [2019] XLNet
----
- [2019 NeurIPS] Xlnet: Generalized autoregressive pretraining for language understanding

## [2019] RoBERTa
---
- [2019] Roberta: A robustly optimized bert pretraining approach

## [2020] ALBERT
---
- [2020 ICLR] Albert: Alite bert for self-supervised learning of language representations

## [2020] DeBERTa
---
- [2020] DeBERTa: Decoding-enhanced BERT with Disentangled Attention

## [2021] LoRA,  Low-Rank Adaptation
---
- [2021] LoRA: Low-Rank Adaptation of Large Language Models

## ChatGLM
----
- https://github.com/THUDM/ChatGLM-6B
- https://chatglm.cn


