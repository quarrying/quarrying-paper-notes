## CoQA
-----

## GLUE
-----

## decaNLP
-----

## WMT 2014 English-to-German
---

## WMT 2014 English-to-French
----

## WuDaoCorpora, WuDao
----
中文语料库.

- [2021] WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models
- https://github.com/THUDM/Chinese-Transformer-XL
- https://data.baai.ac.cn/details/WuDaoCorporaText

## C4, colossal clean crawled corpus
---

## CLUECorpus2020 
----

## THUCTC2
---
> THUCTC2 is a Chinese text classification toolkit accompanying by a 2.19 GB dataset containing 740,000 news documents. 

- https://github.com/thunlp/THUCTC
- http://thuctc.thunlp.org/

## Pile
---
- [2020] The Pile: An 800GB Dataset of Diverse Text for Language Modeling


## RedPajama-Data
----
- https://github.com/togethercomputer/RedPajama-Data


## [awesome-nlp-chinese-corpus](https://github.com/wangmuy/awesome-nlp-chinese-corpus )
---

## [Chinese-Names-Corpus](https://github.com/wainshine/Chinese-Names-Corpus )
---

## [2013 EMNLP] SST, Stanford Sentiment Treebank
---
> The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.
> Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.

- [2013 EMNLP] Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
