- 2017
----

Deep compression算法是比较有应用前景的模型压缩算法, 其分为如下三个步骤: 
![](<[2016 ICLR] Deep Compression_ Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding/arch.png>)

1) 网络剪枝

设置一个阈值, 将绝对值小于阈值的参数置为 0, 对非零参数再次训练. 这一步可以使 AlexNet 和 VGG-16 的模型分别降至原来的 1/9 和 1/13. 可以利用 CSC (compressed sparse column) 或 CSR (compressed sparse row) 等稀疏矩阵存储格式来保存模型参数. 为了进一步压缩, 不是保存索引的绝对位置, 而是保存索引的差分值, 这样可以减少表示索引所需要的位数. 网络剪枝除了可以减少网络复杂度, 还可以视作一种正则化方法, 减少了过拟合. 

2) 参数量化和权重共享

参数量化用来减少每个参数所需要的位数, 权重共享强制多个连接具有相同的权重. 对于 AlexNet, 在不损失精度的情况下, 这一步可以将卷积层参数量化到8位, 全连接层参数量化到5位. 权重共享的基本步骤为: 利用 K-Means 算法对各层所有连接的参数进行聚类, 保持同一聚类内的连接的参数相等, 对网络进行精调. 在反向传播时, 属于同一聚类的连接的梯度进行累加, 作用到同一聚类的权重聚类中心上. 最后, 用聚类中心的权重和属于该聚类的参数的索引替代原有参数. 

3) Huffman 编码

观察到网络参数和索引值的分布都是比较偏离于均匀分布的, 于是可以利用 Huffman 编码对其进行可变长编码. 论文提到, 这一步可以节约 20% 至 30% 的存储空间. 

这篇论文的作者后来又设计了一个专用的硬件设备 Efficient Inference Engine (EIE), 来加速网络压缩. 在 9 个 DNN benchmark 上测试得出, EIE 比 CPU 和 GPU 分别有约 189 倍和 13 倍的计算加速. 
