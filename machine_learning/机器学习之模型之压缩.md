# 综合

## Survey
---
- [Awesome LLM Compression](https://github.com/HuangOwen/Awesome-LLM-Compression)

# Distillation

## Awesome Knowledge Distillation
---
- https://github.com/dkozlov/awesome-knowledge-distillation

## [2014 NIPS] Do Deep Nets Really Need to be Deep
---

## [2014 NIPS] Distilling the Knowledge in a Neural Network
---

## [2015 ICLR] FitNets_ Hints for Thin Deep Nets
---

## [2016] Deep Model Compression_ Distilling Knowledge from Noisy Teachers
---

## [2016 AAAI] Face Model Compression by Distilling Knowledge from Neurons
---

## [2017 CVPR] A Gift from Knowledge Distillation_ Fast Optimization, Network Minimization and Transfer Learning
---

## [2017 ICLR] Paying More Attention to Attention_ Improving the Performance of Convolutional Neural Networks via Attention Transfer
---

## [2017] Efficient Knowledge Distillation from an Ensemble of Teachers
---

## [2017] Like What You Like_ Knowledge Distill via Neuron Selectivity Transfer
---

## [2018 AAAI] Rocket Launching_ A Universal and Efficient Framework for Training Well-performing Light Net
---

## [2018] Feature Matters_ A Stage-by-Stage Approach for Knowledge Transfer
---

## [2018 ECCV] Self-supervised knowledge distillation using singular value decomposition
---

## [2019 AAAI] Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons
---

## [2019] Feature Fusion for Online Mutual Knowledge Distillation
---

## [2019] Knowledge Distillation via Route Constrained Optimization
---

## [2019] Relational Knowledge Distillation
---

## [2019] Triplet Distillation for Deep Face Recognition
---

## [2020] Knowledge Distillation Meets Self-Supervision
---

## [2020] Channel Distillation_ Channel-Wise Attention for Knowledge Distillation
----

## [2016 CVPR] Cross Modal Distillation for Supervision Transfer
---
teacher 模型和 student 模型的输入不是同一个模态.
Settings: 现有配对的一个数据集, 其中一个属于模态 A, 一个属于模态 B. 模态 A 有额外的大量标注数据.

- 利用额外的模态 A 数据训练得到预训练模型.
- 用预训练模型初始化 teacher 模型和 student 模型.
- 在配对数据集上按蒸馏方式训练, 得到的 student 模型作为模态 B 的初始化模型.

## [2022] Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation
---
teacher model 和 student model 结构是一样的, 其中 teacher model 是由 contrastive learning 或 image-text matching 训练得到的, student model 是随机初始化的.

- 在 feature map 上使用蒸馏, 蒸馏损失为 Smooth l1
- 对 teacher feature map 先进行 non-parametric layer normalization
- student model 使用 shared RPB

## [2023] Distilling Step-by-Step
----
把一些未标注的数据通过 CoT 的方式 LLM 生成 label 与 rationale, 把得到的数据在小模型中 finetune

- given an LLM and an unlabeled dataset, prompt the LLM to generate output labels along with rationales to justify the labels. 
- leverage these rationales in addition to the task labels to train smaller downstream models.

> Rationales are natural language explanations that provide support for the model’s predicted label. 

- [2023] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes


# Quantization

## Overview
---
- http://chenrudan.github.io/blog/2018/10/02/networkquantization.html
- [2017] A Survey of Model Compression and Acceleration for Deep Neural Networks

## Deep Compression
---
- [2016 ICLR] Deep Compression_ Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding

## [2015 NIPS] BinaryConnect: Training Deep Neural Networks with binary weights during propagations
---
BinaryConnect 强制权重只能取 +1 和 -1, 这样既减少了模型大小 (是原来模型的 1/32, 如果原来模型用单精度数值保存的话) , 还避免了乘法运算 (在 CPU 和 GPU 上一般会有 2 倍的理论加速).

- https://github.com/MatthieuCourbariaux/BinaryConnect

## [2016] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1
---
Binarized Neural Networks (BNN) 不仅二值化了权重, 还二值化了卷积层和全连接层的输入 (或称 activation) , 在 GPU 上可以达到 7 倍加速.


## [2016] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Network
---
- http://allenai.org/plato/xnornet

## [2017] Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices
---

## Song Han et al.
---
- [2015 NIPS] Learning both Weights and Connections for Efficient Neural Networks

## EIE
---
- [2016] EIE_ Efficient Inference Engine on Compressed Deep Neural Network

## Integer-Arithmetic-Only
---
- [2017] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference

## caffe-int8-convert-tools
---
- https://github.com/BUG1989/caffe-int8-convert-tools

## Ristretto
---
- [Ristretto Overview](http://lepsucd.com/?page_id=621)
- [Ristretto Github Repo](https://github.com/pmgysel/caffe)

## [2015 ICML] Compressing neural networks with the hashing trick
---

## [1993] Svd-net: an algorithm that automatically selects network structure
---

## [2015] Fast algorithms for convolutional neural networks
---

## [2014] Minimizing Computation in Convolutional Neural Networks
---

# Pruning

## [[2018] Progressive Deep Neural Networks Acceleration via Soft Filter Pruning](https://arxiv.org/pdf/1808.07471.pdf)
---

## [[2018] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks](https://arxiv.org/pdf/1808.06866.pdf)
---

## [[2017] To prune, or not to prune: exploring the efficacy of pruning for model compression](https://arxiv.org/pdf/1710.01878.pdf)
---

## [2018] Rethinking the Value of Network Pruning
---

## [2018] Pruning neural networks: is it time to nip it in the bud?
---

## [2018] Dynamic Channel Pruning_ Feature Boosting and Suppression
---

## Awesome Pruning
---
- https://github.com/he-y/Awesome-Pruning


## [2022] FlashAttention
---
- [2022] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

