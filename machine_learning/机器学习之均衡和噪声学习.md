# 均衡学习, 长尾分布

## [2002] Smote: synthetic minority over-sampling technique
----
Data re-sampling

## [2005] Borderline-smote: a new over-sampling method in imbalanced data sets learning
----
Data re-sampling

## [2016 CVPR] Learning deep representation for imbalanced classification
----
Data re-sampling

## [2019 NeurIPS] Learning imbalanced datasets with label-distribution-aware margin loss
---
Data re-sampling

## [2016 CVPR] Training Region-based Object Detectors with Online Hard Example Mining
----
> bootstrapping (and now often called hard negative mining), has existed for at least 20 years. Bootstrapping was introduced in the work of Sung and Poggio [30] in the mid-1990’s (if not earlier) for training face detection models. Their key idea was to gradually grow, or bootstrap, the set of background examples by selecting those examples for which the detector triggers a false alarm. This strategy leads to an iterative training algorithm that alternates between updating the detection model given the current set of examples, and then using the updated model to find new false positives to add to the bootstrapped training set. The process typically commences with a training set consisting of all object examples and a small, random set of background examples.

## [2021 CVPR] Equalization Loss v2 
----
Gradient re-weighting

- [2021 CVPR] Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection

## [2020] Feature augmentation
---
- [2020] Feature space augmentation for long-tailed data

## [2019] Decoupling training
----
- [2019] Decoupling representation and classifier for long-tailed recognition

## [2021] LAS, Label-aware Smoothing
----
- [2021] Improving Calibration for Long-Tailed Recognition


# 噪声学习

## [2022] Confident learning, CL, cleanlab, 置信学习, 可信学习
----
Confident learning 直接利用模型预测结果进行去噪, 解耦了模型和数据清洗的过程, 可以适用于各种可以输出预测概率的模型. 其主要步骤为: 
1) 通过交叉验证的方式计算数据集中每个样本的训练集外预测概率 (Out-of-Sample Predicted Probabilities, 即用没有该样本的训练集训练的模型在该样本上得到的预测概率);
2) 估计噪声标签和真实标签的置信联合矩阵 (Confident Joint) 及置信联合分布矩阵 (即置信联合矩阵的归一化) . 置信联合矩阵与混淆矩阵的不同之处是前者通过引入自适应的阈值提高了其对类间概率分布差异和类间样本量不平衡的健壮性;
3) 基于联合分布矩阵和各样本的噪声标签及预测概率, 并采用一定的策略滤除噪声. 

### Out-of-sample and in-sample predicted probabilities 
> Out-of-sample predicted probabilities refer to the model's probabilistic predictions made only on datapoints that were not shown to the model during training. In contrast, in-sample predicted probabilities on the model's training data will often be way overconfident and cannot be trusted.

### References
- https://cleanlab.ai/
- https://github.com/cleanlab/cleanlab
- [2022] Confident Learning_ Estimating Uncertainty in Dataset Labels

## [2021] Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks
----

## [2020] A survey on deep learning with noisy labels: How to train your model when you cannot trust on the annotations?
----

## [2022] ProMix
----
- [2022] ProMix: Combating Label Noise via Maximizing Clean Sample Utility

