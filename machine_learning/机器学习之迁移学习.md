关键词: 微调 (fine-tuning), 迁移 (transfer), 适应 (adaptation).

## [2022] Towards Online Domain Adaptive Object Detection
---
本文提出了一种 Online Source-Free Domain Adaptation (Online-SFDA) 的方法.

文中提到的域适应的概念:
- UDA (Unsupervised Domain Adaptation): assume that both labelled source data and unlabeled target data are available during adaptation.
- SFDA (Source-Free Domain Adaptation): a source-trained model is adapted towards the target domain without requiring access to the source data.
- Online-SFDA: where a model is adapted to any distribution shifts encountered during deployment in an online manner.
- test-time adaptation: adaptation is performed during test-time.

UDA 的方法可以分为 3 类: adversarial training, self-training and image-to-image translation.


## [2020 ICML] Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation
----

## [2020 CVPR] Model adaptation: Unsupervised domain adaptation without source data
----

## [2018 CVPR] MCD, Maximum classifier discrepancy
----
- [2018 CVPR] Maximum classifier discrepancy for unsupervised domain adaptation

## [2019 AAAI] DVR, Disentangled Variational Representation
----
- [2019 AAAI] Disentangled Variational Representation for Heterogeneous Face Recognition

## [2019 ICML] Parameter-efficient transfer learning for nlp
----

## [2019 ICML] Bert and pals: Projected attention layers for efficient adaptation in multi-task learning
----

## [2020] VTAB, visual task adaptation benchmark
----
- [2020] A large-scale study of representation learning with the visual task adaptation benchmark

## [2023 ICLR] ViT-Adapter
----
在对下游任务进行微调时, 通过额外的架构引入归纳偏差.

- [2023 ICLR] Vision Transformer Adapter for Dense Predictions
- https://github.com/czczup/ViT-Adapter

## [2021] LoRA, Low-Rank Adaptation
----
冻结预训练模型的权重, 并将可训练的秩分解矩阵注入 Transformer 架构的每一层与之并行.

论文作者只研究了对注意力层中的 query, key, value, output 投影矩阵进行秩分解, 如下:
> We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work.

- [2021] LoRA: Low-Rank Adaptation of Large Language Models

## [2021] Prefix Tuning
----
- [2021] Prefix-Tuning: Optimizing Continuous Prompts for Generation

## [2021] Prompt Tuning
----
- [2021] The Power of Scale for Parameter-Efficient Prompt Tuning

## [2022] Visual Prompt Tuning, VPT
----
- [2022 ECCV] Visual Prompt Tuning

## [2023] AdaLoRA
----
- [2023] Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning

## Cross-modal adaptation
----
- [2023] Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models

## Side-tuning
---
- [2020] Side-tuning: a baseline for network adaptation via additive side networks

## [2022] CoOp
----
- [2022] Learning to Prompt for Vision-Language Models

## [2022] CoCoOp
---
- [2022] Conditional Prompt Learning for Vision-Language Models

