**OPINIONS**:
1) 个人总结, 学习率调整可以分为如下方式:
- 基于策略的学习率调整, 如: cosine learning rate decay
- 基于统计的学习率调整, 如: ADAM
- 基于梯度的学习率调整, 如: hypergradient descent

## [2017 ICLR] cosine learning rate decay
---
- [2017 ICLR] SGDR_ Stochastic gradient descent with warm restarts
- [2019 CVPR] Bag of tricks for image classification with convolutional neural networks

## [2017] LARS, Layer-wise Adaptive Rate Scaling
---
当 batch size 很大时, 采用传统方法 (linear learning rate scaling with warm-up) 训练的模型, 精度会下降. 于是提出了 LARS ( Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.)

> There are two notable differences between LARS and other adaptive algorithms such as ADAM (Kingma &
> Ba (2014)) or RMSProp (Tieleman & Hinton (2012)): first, LARS uses a separate learning rate for
> each layer and not for each weight, which leads to better stability. And second, the magnitude of the
> update is controlled with respect to the weight norm for better control of training speed.

- [2017] Large batch training of convolutional networks

## [2014] linear learning rate scaling 
---
batch size 增加 k 倍, 则 LR 相应的增加 k 倍, 同时保持其他超参 (如 momentum, weight decay 等) 不变.

> It was observed that linear scaling works much better for networks with Batch
> Normalization (e.g. Codreanu et al. (2017)).

- [2014] One weird trick for parallelizing convolutional neural networks

## [2017] warm-up
---
- [2017] Accurate, large minibatch sgd: Training imagenet in 1 hour

## [2018 ACL] STLR, Slanted Triangular Learning Rates
---
Slanted Triangular Learning Rates (STLR) is a learning rate schedule which first linearly increases the learning rate and then linearly decays it, which can be seen in Figure to the right. It is a modification of Triangular Learning Rates, with a short increase and a long decay period.

- [2018 ACL] Universal Language Model Fine-tuning for Text Classification

## [2017] Hypergradient Descent
--- 
- [2017] Online Learning Rate Adaptation with Hypergradient Descent


## [2020] ZeRO, Zero Redundancy Optimizer 
---
- [2020] ZeRO_ Memory Optimizations Toward Training Trillion Parameter Models


## [2019 ICLR] AdamW
---
- [2019 ICLR] Decoupled weight decay regularization

## Layer-wise learning rate decay (LLRD)
----
> The LLRD assigns different learning rates to each layer of the model backbone. It sets a large learning rate for the top (last) layer and uses a multiplicative decay rate to decrease the learning rate layer-by-layer from top (last) to bottom (first). With high learning rates, the features recognized by the top layers change more and adapt to new tasks more easily, while the bottom layers have low learning rates and more easily preserve the features learned during pre-training.

## [2022] GACT
---
- [2022] GACT: Activation Compressed Training for Generic Network Architectures

## [2016] Gradient Checkpointing
---
以时间换空间.

- [2016] Training Deep Nets with Sublinear Memory Cost
